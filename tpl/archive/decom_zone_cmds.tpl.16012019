Remove a zone
1 Description

This procedure describes how to remove a Solaris 10 container
2 Prerequisites

# If archiving is needed, stop the applications and open a ticket to SBA-OP to archive the zone
2.1 on primary source node, disable the application

export zone_name=<zone_name>

ssh ${zone_name}
for application in `ls /applications | grep -v wood | sed "s/\///g"`; do /applications/${application}/users/system/init.d/${application} status 2>/dev/null; done | grep -v STATE
for application in `ls /applications | grep -v wood | sed "s/\///g"`; do /applications/${application}/users/system/init.d/${application} disable 2>/dev/null; done
for application in `ls /applications | grep -v wood | sed "s/\///g"`; do /applications/${application}/users/system/init.d/${application} status 2>/dev/null; done | grep -v STATE
exit

2.2 Open an SMT ticket to SBA-OP to archive the zone for 3 years (or 7 years for financial products).

Ticket: 

3 Instructions
3.1 variables, on both global zones

# define the env variables on both globalzones

export zone_name=<zone_name>
decom_zone_set_vars <zone_name>
cmdb zone | awk -F";" '/'$zone_name'/ {print $8": ", $7}' | xargs
cd $tmp_folder
OR
export tmp_folder=/net/vdm-unix/systemstore/temp/${zone_name}
[ ! -d $tmp_folder ] && mkdir $tmp_folder
who=`who am i | awk '{print $1}'`

3.2 If the zone rg is in the unmanaged state, then put it bask in managed state

clrg manage ${zone_name}-rg
clrg online -e ${zone_name}-rg

3.3 inform integration, db teams
3.4 schedule dowtine in centreon
3.5 get OS on primary node

os=`zlogin $zone_name uname -r | sed -e 's/^5\./Solaris /'` && echo $os

clrs list -g ${zone_name}-rg | xargs

clrs show -p Zpools ${zone_name}-zfs | grep Zpools


3.6 get network information, on primary node

{
global_zone_os=`uname -r`
if [ x"$global_zone_os" == x'5.10' ]; then
  zonecfg -z $zone_name info net | grep address: | awk '{print $2}' | awk -F'/' '{print $1}' | while read ip
    do
      name=`nslookup $ip | grep name | awk '{print $NF}' | sed -e 's/\.$//'`
      echo ${ip}: ${name}
    done > ${tmp_folder}/network_ip.txt
else
  zonecfg -z $zone_name info anet| grep allowed-address | grep -v configure-allowed-address | awk '{print $2}' | perl -pe 's/,/\n/g' | awk -F'/' '{print $1}' | while read ip
    do
      name=`nslookup $ip | grep name | awk '{print $NF}' | sed -e 's/\.$//'`
      echo ${ip}: ${name}
    done > ${tmp_folder}/network_ip.txt
fi
cat /zones/${zone_name}/root/etc/hosts >${tmp_folder}/network_etc_hosts.txt
cat ${tmp_folder}/network_ip.txt
}


3.6.1 (3.22) remove client monitoring

{
cat <<EOT
Bonjour,

Voulez-vous supprimer les clients suivants du monitoring:
`cat ${tmp_folder}/network_ip.txt | awk '{print $2}' | sed -e 's/.opoce.cec.eu.int//' | grep -v "bkp-${zone_name}"`

Merci d'avance
EOT
} | mailx -s "suppression du client monitoring pour $zone_name" -r $who -c $who,OPDL-INFRA-INT-PROD@publications.europa.eu Michel.MOMMATI@ext.publications.europa.eu


3.7 get storage information, on both nodes

/home/admin/bin/storage_info.pl -A > ${tmp_folder}/storage_info_`uname -n`.txt && ls -lh ${tmp_folder}/storage_info_`uname -n`.txt

3.8 get zone storage information on primary node

export zpools=`zonecfg -z $zone_name info dataset | grep name | awk '{print $2}' | awk -F'/' '{print $1}'` 
zpool status $zpools | egrep -v errors|grep .

{
> ${tmp_folder}/storage_hex_lun_id.txt
> ${tmp_folder}/storage_type.txt
for pool in $zpools
do
  zpool status $pool | grep ONLINE | egrep -v "state:|$pool|mirror" | awk '{print $1}' | sed -e 's/s0$//' -e 's/s2$//' | while read disk
  do
    [ `echo "$disk" | grep -c '^emcpower'` -gt 0 ] && disk=`echo $disk | sed -e 's/c$/a/'`
    line=$(grep "$disk " ${tmp_folder}/storage_info_`uname -n`.txt)
    [ `echo "$line" | egrep -c 'VNX'` -gt 0 ] && export storage_type=VNX
    [ `echo "$line" | egrep -c 'VMAX_2560|VMAX_3453'` -gt 0 ] && export storage_type=VMAX
    [ `echo "$line" | egrep -c 'Vmax3'` -gt 0 ] && export storage_type=Vmax3
    hex_lun_id=`echo "$line" | awk '{print $9}'`
    grep `echo "$hex_lun_id" | uniq` ${tmp_folder}/storage_info_`uname -n`.txt | grep $storage_type
    echo "$hex_lun_id" | uniq >> ${tmp_folder}/storage_hex_lun_id.txt
    echo "$storage_type" >> ${tmp_folder}/storage_type.txt
  done 
done
}>${tmp_folder}/storage_info_${zone_name}.txt
cat ${tmp_folder}/storage_info_${zone_name}.txt
cat ${tmp_folder}/storage_hex_lun_id.txt | sort -u | tee ${tmp_folder}/storage_hex_lun_id.txt 
cat ${tmp_folder}/storage_hex_lun_id.txt

3.9 get zone storage information on secondary node

{
for id in `cat ${tmp_folder}/storage_hex_lun_id.txt | sort -u`
do
    grep "$id " ${tmp_folder}/storage_info_`uname -n`.txt | grep `cat ${tmp_folder}/storage_type.txt | sort -u` 
done
} >> ${tmp_folder}/storage_info_${zone_name}.txt
cat ${tmp_folder}/storage_info_${zone_name}.txt

3.10 On primary node, get disks WWNs of the zone

{
storage_type=`cat ${tmp_folder}/storage_type.txt | grep -v '^$'| sort -u`
case "$storage_type" in
  VMAX)
    cat ${tmp_folder}/storage_info_${zone_name}.txt | awk '{print $18}' | sort -u | while read symdevice
    do
      for SID in 000292603453 000292602560
        do 
          symdev show -sid $SID $symdevice| grep 'Device WWN' | awk '{print $4;exit}'
        done
    done
  ;;
  Vmax3)
    cat ${tmp_folder}/storage_info_${zone_name}.txt | awk '{print $28}' | sort -u | while read symdevice
    do
       for SID in 000296700060 000296700069
        do 
          symdev show -sid $SID $symdevice| grep 'Device WWN' | awk '{print $4;exit}'
        done
    done
  ;;
  VNX)
    cat ${tmp_folder}/storage_info_${zone_name}.txt | awk '{print $28} | sort -u'
  ;;
esac
} | tee ${tmp_folder}/wwn.txt 

==========================================================================================================
If we do not have powerpath installed, we do not get the LUN id
Alternative : use cldev

for DID in `cat ${tmp_folder}/storage_info_${zone_name}.txt| awk '{print $29}'`; do  cldev show -v $DID  | egrep -iv ascii | awk -F":" '/Disk ID/ {print $NF}'| xargs;done  | tee ${tmp_folder}/wwn.txt

==========================================================================================================


# Get the ASM disks IDs if any

get_asm_disk_ids $zone_name | tee -a ${tmp_folder}/wwn.txt
get_asm_lun_hex | tee -a ${tmp_folder}/storage_hex_lun_id.txt

========================================================
===> W A I T until the long term backup is finished <===
========================================================

3.11 on primary node, stop the zone

zoneadm -z $zone_name halt && zoneadm -z $zone_name list -v

3.12 on primary node, unconfigure the cluster resources for the zone

{
echo "# ${zone_name}-rg"
RS=`clrs list -g ${zone_name}-rg| xargs` && echo "# $RS"
echo clrs disable $RS
echo clrs delete $RS
echo clrg offline ${zone_name}-rg 
echo clrg delete ${zone_name}-rg
}

3.13 on both nodes, unconfigure the zone

zonecfg -z $zone_name delete -F

3.14 on primary node, destroy zpools

echo $zpools
{
for pool in $zpools
do
  echo "Importing pool: $pool"
  zpool import $pool
  zpool destroy $pool && echo $pool destroyed
done
}

3.15 on primary node, remove disk group, if any
cldg list | grep $zone_name
cldg offline ${zone_name} && cldg delete ${zone_name} && cldg list | grep $zone_name

Note: 
On old systems, one might have several DG's for one zone 
cldg list | grep $zone_name

===================
Note:
# One need to cleanup the replicated_devices file by unbinding the binded DID's
# If the DG is already removed:
DIDLIST=`cat /etc/cluster/ccr/global/replicated_devices| grep $zone_name | awk '{print $1}'` && echo $DIDLIST
for DID in $DIDLIST; do echo "scdidadm -b $DID && /usr/cluster/bin/scgdevs";done
grep -c $zone_name /etc/cluster/ccr/global/replicated_devices

# If the DG is still present:
DG=`cldg list | grep $zone_name` && echo $DG
[ ! -z $DG ] && DGDIDs=`cldg show -v $DG| awk -F":" '/device names/ {print $2}'| xargs | sed 's:/dev/did/rdsk/::g;s:s2::g'` && echo $DGDIDs && cldev show -v $DGDIDs

# Return the replicated DID instance to its prior state of being two separate DID instances.

for DID in `echo $DGDIDs| sed 's/d//g'`; do echo "scdidadm -b $DID && echo /usr/cluster/bin/scgdev";done
grep -c $zone_name /etc/cluster/ccr/global/replicated_devices

===================

3.16 on both nodes, put the disks offline, one node after the other one (not at same time)

# Offline the LUNs

# Note: the script only generates the commands that we have to execute, after double checking.

{
for id in `cat ${tmp_folder}/storage_hex_lun_id.txt  | sort -u`
do
  if [ x"$global_zone_os" == x'5.10' -o ! -f /etc/powermt ]; then
    grep " $id " ${tmp_folder}/storage_info_`uname -n`.txt | grep `cat ${tmp_folder}/storage_type.txt| sort -u` | awk '{print "luxadm -e offline /dev/rdsk/"$3"s2"}'
  else
    grep " $id " ${tmp_folder}/storage_info_`uname -n`.txt | grep `cat ${tmp_folder}/storage_type.txt| sort -u` | awk '{print "/home/admin/bin/op_dev_offline_powermt_luxadm.sh "$3}'
  fi
done
} | sh 

# Unconfigure the removed LUNs

cleanupLUNs

# OR manual way:
{
uncfglist=`cfgadm -al -o show_SCSI_LUN | nawk -F"[ ,]" '/unusable/ {print $1}' | sort -u | xargs`
if [ ! -z $uncfglist ]; then
   for i in $uncfglist; do echo "cfgadm -c unconfigure -o unusable_SCSI_LUN $i"; done
else
   echo "===> Nothing to cleanup"
fi
}
# Cleanup the /dev & /devices namespaces
devfsadm -Cv
# Remove obsolete device IDs references
cldev clear

# Post-Check

cldev status -s fail

[ -f /etc/powermt ] && /etc/powermt display && /etc/powermt check

3.17 remove puppet client configuration

For puppet, delete the host from the Foreman GUI : https://foreman/users/login
3.18 Open an SMT ticket to SBA-OP to remove the backup client

{
echo "#SMT Title: remove backup client for bkp-${zone_name}"
echo "#SMT Template: BACKUP REQUEST - Delete client"
echo
echo Client name: bkp-$zone_name
echo OS: $os
echo Reason: zone removed
echo; echo Merci
} | mailx -s "create a ticket with this content" $who

3.19 Open an SMT ticket to SBA-OP to recover the storage

{
echo "#SMT Title: recover storage for ${zone_name}"
echo "#SMT Template: STORAGE REQUEST - Retrieve unused storage"
echo
echo "Type of storage (VNX - VMAX - VMAX3 - NAS - eNAS): `cat ${tmp_folder}/storage_type.txt | sort -u`"
echo "Impacted hosts: `clnode list | xargs| perl -pe 's/ /, /'`"
echo "Masking info (vm, datastore, zone,... name): $zone_name"
echo "LUN WWN and/or ID:
`cat ${tmp_folder}/wwn.txt`"
echo; echo Merci
} | mailx -s "create a ticket with this content" $who

3.20 Request DB team to remove the RMAN backup client, if any

{
echo "La zone $zone_name n'existe plus, les clients RMAN peuvent etre supprimes."
echo Merci.
} | mailx -s "remove rman client: ${zone_name}" -r $who -c $who OP-INFRA-DB@publications.europa.eu

3.21 network: free up the zone IP's

Instructions:
cat ${tmp_folder}/network_ip.txt
For the opsrvxx entries, double-check that the opsrv IP @ still matches the configured IP's
Go to https://resop/ip, and complete the template to delete all zone and opsrv entries
Select "Delete" in the Type field
Enter the fqdn in the Record Name field, and click on the "IP" icon, the record value should show up

3.23 change status in CMDB to archived

{
echo "La zone $zone_name n'existe plus, elle peut etre supprimee de la CMDB."
echo Merci.
} | mailx -s "mise a jour de la cmdb: ${zone_name}" -r $who -c $who OP-INFRA-OPENSYSTEMS-CHGMGT@publications.europa.eu


