##############################################################################################################################################
# shrink avec send/receive de l'ancien vers le nouveau zpool
##############################################################################################################################################

6:34/root@bart # zpool status ceres_tz_NEW
  pool: ceres_tz_NEW
 state: ONLINE
 scrub: none requested
config:

        NAME                                     STATE     READ WRITE CKSUM
        ceres_tz_NEW                             ONLINE       0     0     0
          c4t600A0B800026676A000017E44CB76009d0  ONLINE       0     0     0
          c4t600A0B800026676A000017E54CB76141d0  ONLINE       0     0     0
          c4t600A0B800026677200001A2A4CB7696Ad0  ONLINE       0     0     0

errors: No known data errors


6:34/root@bart # zpool list               
NAME           SIZE   USED  AVAIL    CAP  HEALTH  ALTROOT
ceres_tz       436G   310G   126G    71%  ONLINE  -
ceres_tz_NEW   357G  79.5K   357G     0%  ONLINE  -
rpool         33.8G  11.3G  22.5G    33%  ONLINE  -



6:34/root@bart # zpool status ceres_tz    
  pool: ceres_tz
 state: ONLINE
 scrub: none requested
config:

        NAME                                     STATE     READ WRITE CKSUM
        ceres_tz                                 ONLINE       0     0     0
          c4t600A0B800048F43A000016894BC5A4C6d0  ONLINE       0     0     0

errors: No known data errors


##### variables
NEWPOOL=ceres_tz_NEW
POOLTOBACKUP=ceres_tz
CURDATE=20101015_0630
NEWDATE=_09h00
export NEWPOOL POOLTOBACKUP CURDATE
echo "$NEWPOOL $POOLTOBACKUP $CURDATE $NEWDATE"
ceres_tz_NEW ceres_tz 20101015_0630 _09h00



##### snapshot de POOLTOBACKUP
zfs snapshot -r ${POOLTOBACKUP}@${CURDATE}
zfs list -r $POOLTOBACKUP

NAME                                                   USED  AVAIL  REFER  MOUNTPOINT
ceres_tz                                               310G   119G    20K  /zpool/ceres_tz
ceres_tz@20101015_0630                                    0      -    20K  -
ceres_tz/applications                                  246G   119G  24.5K  none
ceres_tz/applications@20101015_0630                       0      -  24.5K  -
ceres_tz/applications/cdl                              717M   119G  25.5K  none
ceres_tz/applications/cdl@20101015_0630                   0      -  25.5K  -
ceres_tz/applications/cdl/users                        717M   119G   717M  /applications/cdl/users
ceres_tz/applications/cdl/users@20101015_0630             0      -   717M  -
ceres_tz/applications/ceres                            242G   119G  24.5K  none
ceres_tz/applications/ceres@20101015_0630                 0      -  24.5K  -
ceres_tz/applications/ceres/orabin                    3.57G   119G  3.57G  /applications/ceres/orabin
ceres_tz/applications/ceres/orabin@20101015_0630          0      -  3.57G  -
ceres_tz/applications/ceres/oradata                   95.9G   119G  95.9G  /applications/ceres/oradata
ceres_tz/applications/ceres/oradata@20101015_0630      309K      -  95.9G  -
ceres_tz/applications/ceres/oralog                    23.8M   119G  23.8M  /applications/ceres/oralog
ceres_tz/applications/ceres/oralog@20101015_0630          0      -  23.8M  -
ceres_tz/applications/ceres/users                      142G   119G   142G  /applications/ceres/users
ceres_tz/applications/ceres/users@20101015_0630           0      -   142G  -
ceres_tz/applications/ceres/xchange                   31.1M   119G  31.1M  /applications/ceres/xchange
ceres_tz/applications/ceres/xchange@20101015_0630         0      -  31.1M  -
ceres_tz/applications/legiswrite                      1.08G   119G  25.5K  none
ceres_tz/applications/legiswrite@20101015_0630            0      -  25.5K  -
ceres_tz/applications/legiswrite/users                1.08G   119G  1.08G  /applications/legiswrite/users
ceres_tz/applications/legiswrite/users@20101015_0630      0      -  1.08G  -
ceres_tz/applications/lgw                              733M   119G  25.5K  none
ceres_tz/applications/lgw@20101015_0630                   0      -  25.5K  -
ceres_tz/applications/lgw/users                        733M   119G   733M  /applications/lgw/users
ceres_tz/applications/lgw/users@20101015_0630             0      -   733M  -
ceres_tz/applications/oraagent                        1.38G   639M  1.38G  /u01/oraagent
ceres_tz/applications/oraagent@20101015_0630              0      -  1.38G  -
ceres_tz/zone                                         64.3G   119G  64.3G  /zones/ceres_tz
ceres_tz/zone@20101015_0630                               0      -  64.3G  -


##### send|recieve de POOLTOBACKUP vers NEWPOOL
LAUNCH="06:38"
export LAUNCH
{
TAG="ZFS"
export TAG
-
for Filesystem in $(zfs list -Hr ${POOLTOBACKUP} | grep  $CURDATE | grep -v adadata | awk '{print $1}') 
do 
 echo "logger -p daemon.notice -t $TAG \"FS: $Filesystem\""
 echo "zfs send $Filesystem | zfs receive -dF ${NEWPOOL}"
done
echo "logger -p daemon.notice -t $TAG \"SEND RECEIVE END\""
} | at -q n $LAUNCH



##### disable de l'application
13:23/root@ceres_tz # /applications/ceres/users/system/init.d/ceres disable
13:23/root@ceres_tz # /applications/cdl/users/system/init.d/cdl disable
13:23/root@ceres_tz # /applications/lgw/users/system/init.d/lgw disable


13:24/root@ceres_tz # svcs ceres cdl lgw 
STATE          STIME    FMRI
disabled       11:04:42 svc:/applications/ceres:app
disabled       13:23:15 svc:/applications/ceres:ewopweb
disabled       13:23:15 svc:/applications/ceres:ewop
disabled       13:23:15 svc:/applications/ceres:ewopconslegweb
disabled       13:23:16 svc:/applications/ceres:samba
disabled       13:23:16 svc:/applications/ceres:woodweb
disabled       13:23:20 svc:/applications/ceres:vm2
disabled       13:23:21 svc:/applications/ceres:cdromjo
disabled       13:23:21 svc:/applications/ceres:cl2sf
disabled       13:23:21 svc:/applications/ceres:wood
disabled       13:23:22 svc:/applications/ceres:ceresjur
disabled       13:23:44 svc:/applications/cdl:woodweb
disabled       13:23:47 svc:/applications/cdl:wood
disabled       13:23:50 svc:/applications/cdl:app
disabled       13:24:02 svc:/applications/ceres:ora
disabled       13:24:07 svc:/applications/lgw:woodweb
disabled       13:24:10 svc:/applications/lgw:wood
disabled       13:24:13 svc:/applications/lgw:app


##### recupere les parametres zfs de POOLTOBACKUP pour donner plus tard a NEWPOOL
{
for PARAM in mountpoint quota reservation recordsize compression  aclmode aclinherit zoned 
do
  for Filesystem in ` zfs list -Hr $POOLTOBACKUP | grep -v $CURDATE   | awk '{print $1}'`
  do
    zfs get -H $PARAM $Filesystem | perl -p -e 's:$ENV{POOLTOBACKUP}:$ENV{NEWPOOL}:'| awk '{print "zfs set "$2"="$3" "$1}'
  done
done
} | tee  /home/betorma/xchange/PARAM_ZFS_$POOLTOBACKUP.txt


################ ARRET ZONE

13:24/root@ceres_tz # init 0
13:26/root@bart # zoneadm list -ivc
  ID NAME             STATUS     PATH                           BRAND    IP    
   0 global           running    /                              native   shared
   - ceres_tz         installed  /zones/ceres_tz                native   shared

13:32/root@bart # umount /zones/ceres_tz 

###############  LAST SNAP

NEWPOOL=ceres_tz_NEW
POOLTOBACKUP=ceres_tz
CURDATE=20101015_0630
NEWDATE=_14h00
export NEWPOOL POOLTOBACKUP CURDATE
echo "$NEWPOOL $POOLTOBACKUP $CURDATE $NEWDATE"

ceres_tz_NEW ceres_tz 20101015_0630 _14h00



zfs snapshot -r ${POOLTOBACKUP}@${CURDATE}${NEWDATE}
zfs list -r $POOLTOBACKUP
NAME                                                         USED  AVAIL  REFER  MOUNTPOINT
ceres_tz                                                     310G   119G    20K  /zpool/ceres_tz
ceres_tz@20101015_0630                                          0      -    20K  -
ceres_tz@20101015_0630_14h00                                    0      -    20K  -
ceres_tz/applications                                        246G   119G  24.5K  none
ceres_tz/applications@20101015_0630                             0      -  24.5K  -
ceres_tz/applications@20101015_0630_14h00                       0      -  24.5K  -
ceres_tz/applications/cdl                                    719M   119G  25.5K  none
ceres_tz/applications/cdl@20101015_0630                         0      -  25.5K  -
ceres_tz/applications/cdl@20101015_0630_14h00                   0      -  25.5K  -
ceres_tz/applications/cdl/users                              719M   119G   717M  /applications/cdl/users
ceres_tz/applications/cdl/users@20101015_0630               1.59M      -   717M  -
ceres_tz/applications/cdl/users@20101015_0630_14h00             0      -   717M  -
ceres_tz/applications/ceres                                  242G   119G  24.5K  none
ceres_tz/applications/ceres@20101015_0630                       0      -  24.5K  -
ceres_tz/applications/ceres@20101015_0630_14h00                 0      -  24.5K  -
ceres_tz/applications/ceres/orabin                          3.59G   119G  3.57G  /applications/ceres/orabin
ceres_tz/applications/ceres/orabin@20101015_0630            23.3M      -  3.57G  -
ceres_tz/applications/ceres/orabin@20101015_0630_14h00          0      -  3.57G  -
ceres_tz/applications/ceres/oradata                         96.0G   119G  95.9G  /applications/ceres/oradata
ceres_tz/applications/ceres/oradata@20101015_0630            123M      -  95.9G  -
ceres_tz/applications/ceres/oradata@20101015_0630_14h00         0      -  95.9G  -
ceres_tz/applications/ceres/oralog                          35.8M   119G  35.8M  /applications/ceres/oralog
ceres_tz/applications/ceres/oralog@20101015_0630              22K      -  23.8M  -
ceres_tz/applications/ceres/oralog@20101015_0630_14h00          0      -  35.8M  -
ceres_tz/applications/ceres/users                            142G   119G   142G  /applications/ceres/users
ceres_tz/applications/ceres/users@20101015_0630             33.9M      -   142G  -
ceres_tz/applications/ceres/users@20101015_0630_14h00           0      -   142G  -
ceres_tz/applications/ceres/xchange                         31.1M   119G  31.1M  /applications/ceres/xchange
ceres_tz/applications/ceres/xchange@20101015_0630             82K      -  31.1M  -
ceres_tz/applications/ceres/xchange@20101015_0630_14h00         0      -  31.1M  -
ceres_tz/applications/legiswrite                            1.08G   119G  25.5K  none
ceres_tz/applications/legiswrite@20101015_0630                  0      -  25.5K  -
ceres_tz/applications/legiswrite@20101015_0630_14h00            0      -  25.5K  -
ceres_tz/applications/legiswrite/users                      1.08G   119G  1.08G  /applications/legiswrite/users
ceres_tz/applications/legiswrite/users@20101015_0630          48K      -  1.08G  -
ceres_tz/applications/legiswrite/users@20101015_0630_14h00      0      -  1.08G  -
ceres_tz/applications/lgw                                    734M   119G  25.5K  none
ceres_tz/applications/lgw@20101015_0630                         0      -  25.5K  -
ceres_tz/applications/lgw@20101015_0630_14h00                   0      -  25.5K  -
ceres_tz/applications/lgw/users                              734M   119G   733M  /applications/lgw/users
ceres_tz/applications/lgw/users@20101015_0630               1.09M      -   733M  -
ceres_tz/applications/lgw/users@20101015_0630_14h00             0      -   733M  -
ceres_tz/applications/oraagent                              1.38G   634M  1.37G  /u01/oraagent
ceres_tz/applications/oraagent@20101015_0630                8.45M      -  1.38G  -
ceres_tz/applications/oraagent@20101015_0630_14h00              0      -  1.37G  -
ceres_tz/zone                                               64.3G   119G  64.3G  /zones/ceres_tz
ceres_tz/zone@20101015_0630                                 20.5M      -  64.3G  -
ceres_tz/zone@20101015_0630_14h00                               0      -  64.3G  -




LAUNCH="13:35"
export LAUNCH
{
TAG="ZFS"
export TAG
for Filesystem in $(zfs list -Hr ${POOLTOBACKUP} | grep  $CURDATE | awk '{print $1}') 
do 
 echo "logger -p daemon.notice -t $TAG \"FS: $Filesystem\""
 echo "zfs send -i $Filesystem ${Filesystem}${NEWDATE}| zfs receive -Fd ${NEWPOOL}"
done
echo "logger -p daemon.notice -t $TAG \"SEND RECEIVE END\""
} | at -q n $LAUNCH


###########" CHECK SIZE
zfs list -r $POOLTOBACKUP | grep -v ${CURDATE}
zfs list -r $NEWPOOL | grep -v ${CURDATE}


##### changement des zpool + zone
zpool export $POOLTOBACKUP

#### changer les parametres ZFS
sh /home/betorma/xchange/PARAM_ZFS_$POOLTOBACKUP.txt

zfs destroy -r ${NEWPOOL}@${CURDATE}${NEWDATE}
zfs destroy -r ${NEWPOOL}@${CURDATE}
zfs list -r ${NEWPOOL}
NAME                                         USED  AVAIL  REFER  MOUNTPOINT
ceres_tz_NEW                                 310G  41.4G    20K  /zpool/ceres_tz
ceres_tz_NEW/applications                    246G  41.4G    26K  none
ceres_tz_NEW/applications/cdl                717M  41.4G    21K  none
ceres_tz_NEW/applications/cdl/users          717M  41.4G   717M  /applications/cdl/users
ceres_tz_NEW/applications/ceres              242G  41.4G    26K  none
ceres_tz_NEW/applications/ceres/orabin      3.57G  41.4G  3.57G  /applications/ceres/orabin
ceres_tz_NEW/applications/ceres/oradata     95.9G  41.4G  95.9G  /applications/ceres/oradata
ceres_tz_NEW/applications/ceres/oralog      35.8M  41.4G  35.8M  /applications/ceres/oralog
ceres_tz_NEW/applications/ceres/users        142G  41.4G   142G  /applications/ceres/users
ceres_tz_NEW/applications/ceres/xchange     31.0M  41.4G  31.0M  /applications/ceres/xchange
ceres_tz_NEW/applications/legiswrite        1.08G  41.4G    21K  none
ceres_tz_NEW/applications/legiswrite/users  1.08G  41.4G  1.08G  /applications/legiswrite/users
ceres_tz_NEW/applications/lgw                733M  41.4G    21K  none
ceres_tz_NEW/applications/lgw/users          733M  41.4G   733M  /applications/lgw/users
ceres_tz_NEW/applications/oraagent          1.37G   642M  1.37G  /u01/oraagent
ceres_tz_NEW/zone                           64.3G  41.4G  64.3G  /zones/ceres_tz

zpool export $NEWPOOL 
zpool import $NEWPOOL  $POOLTOBACKUP

zfs list -r $POOLTOBACKUP 
13:39/root@bart # zfs list -r $POOLTOBACKUP 
NAME                                     USED  AVAIL  REFER  MOUNTPOINT
ceres_tz                                 310G  41.4G    20K  /zpool/ceres_tz
ceres_tz/applications                    246G  41.4G    26K  none
ceres_tz/applications/cdl                717M  41.4G    21K  none
ceres_tz/applications/cdl/users          717M  41.4G   717M  /applications/cdl/users
ceres_tz/applications/ceres              242G  41.4G    26K  none
ceres_tz/applications/ceres/orabin      3.57G  41.4G  3.57G  /applications/ceres/orabin
ceres_tz/applications/ceres/oradata     95.9G  41.4G  95.9G  /applications/ceres/oradata
ceres_tz/applications/ceres/oralog      35.8M  41.4G  35.8M  /applications/ceres/oralog
ceres_tz/applications/ceres/users        142G  41.4G   142G  /applications/ceres/users
ceres_tz/applications/ceres/xchange     31.0M  41.4G  31.0M  /applications/ceres/xchange
ceres_tz/applications/legiswrite        1.08G  41.4G    21K  none
ceres_tz/applications/legiswrite/users  1.08G  41.4G  1.08G  /applications/legiswrite/users
ceres_tz/applications/lgw                733M  41.4G    21K  none
ceres_tz/applications/lgw/users          733M  41.4G   733M  /applications/lgw/users
ceres_tz/applications/oraagent          1.37G   642M  1.37G  /u01/oraagent
ceres_tz/zone                           64.3G  41.4G  64.3G  /zones/ceres_tz


###### on redemarre

[110408/174253]root@mercure# zonecfg -z dlib_pz set limitpriv=default,proc_priocntl
[110408/174506]root@mercure# zoneadm -z dlib_pz boot && zlogin -C dlib_pz 


[110408/174546]root@dlib_pz# svcs -xv
svc:/application/print/server:default (LP print server)
 State: disabled since Fri Apr 08 17:45:13 2011
Reason: Disabled by an administrator.
   See: http://sun.com/msg/SMF-8000-05
   See: man -M /usr/share/man -s 1M lpsched
Impact: 2 dependent services are not running:
        svc:/application/print/rfc1179:default
        svc:/application/print/ipp-listener:default

svc:/system/webconsole:console (java web console)
 State: offline since Fri Apr 08 17:45:25 2011
Reason: Start method is running.
   See: http://sun.com/msg/SMF-8000-C4
   See: man -M /usr/share/man -s 1M smcwebserver
   See: /var/svc/log/system-webconsole:console.log
Impact: This service is not running.




##### demarrage appli 






##### recuperer/ rendre les lun




##############################################################################################################################################
# shrink avec backup de l'ancien zpool, puis restaure sur le nouveau zpool
##############################################################################################################################################

8:51/poseidon@root # zpool list
NAME             SIZE   USED  AVAIL    CAP  HEALTH  ALTROOT
cups_pz         25.2G  2.21G  23.0G     8%  ONLINE  -
dwcom_pz         117G  94.4G  22.6G    80%  ONLINE  -
gescomx_pz       218G   131G  87.4G    59%  ONLINE  -
gescomx_pz_NEW   168G   112K   168G     0%  ONLINE  -

8:51/poseidon@root # zpool status gescomx_pz
  pool: gescomx_pz
 state: ONLINE
 scrub: none requested
config:

        NAME                                     STATE     READ WRITE CKSUM
        gescomx_pz                               ONLINE       0     0     0
          c8t60060480000290103312533030353541d0  ONLINE       0     0     0

errors: No known data errors
8:51/poseidon@root # zpool status gescomx_pz_NEW
  pool: gescomx_pz_NEW
 state: ONLINE
 scrub: none requested
config:

        NAME                                     STATE     READ WRITE CKSUM
        gescomx_pz_NEW                           ONLINE       0     0     0
          c8t60060480000290103312533030313536d0  ONLINE       0     0     0

errors: No known data errors


##### variables
NEWPOOL=gescomx_pz_NEW
POOLTOBACKUP=gescomx_pz
CURDATE=20091025_0850
NEWDATE=_09h00
export NEWPOOL POOLTOBACKUP CURDATE
echo "$NEWPOOL $POOLTOBACKUP $CURDATE $NEWDATE"
gescomx_pz_NEW gescomx_pz 20091025_0850 _09h00


##### snapshot de POOLTOBACKUP
zfs snapshot -r ${POOLTOBACKUP}@${CURDATE}
zfs list -r $POOLTOBACKUP

NAME                                                     USED  AVAIL  REFER  MOUNTPOINT
gescomx_pz                                               131G  84.0G    18K  /zpool/gescomx_pz
gescomx_pz@20091025_04h00                                   0      -    18K  -
gescomx_pz@20091025_0850                                    0      -    18K  -
gescomx_pz/applications                                  129G  84.0G    18K  none
gescomx_pz/applications@20091025_04h00                      0      -    18K  -
gescomx_pz/applications@20091025_0850                       0      -    18K  -
gescomx_pz/applications/gescomx                          129G  84.0G    23K  /applications/gescomx
gescomx_pz/applications/gescomx@20091025_04h00              0      -    23K  -
gescomx_pz/applications/gescomx@20091025_0850               0      -    23K  -
gescomx_pz/applications/gescomx/adadata                 69.8G  84.0G  69.8G  /applications/gescomx/adadata
gescomx_pz/applications/gescomx/adadata@20091025_04h00  1.77M      -  69.8G  -
gescomx_pz/applications/gescomx/adadata@20091025_0850       0      -  69.8G  -
gescomx_pz/applications/gescomx/adaexp                  29.1G  84.0G  29.1G  /applications/gescomx/adaexp
gescomx_pz/applications/gescomx/adaexp@20091025_04h00    148K      -  29.1G  -
gescomx_pz/applications/gescomx/adaexp@20091025_0850        0      -  29.1G  -
gescomx_pz/applications/gescomx/users                   30.4G  84.0G  30.3G  /applications/gescomx/users
gescomx_pz/applications/gescomx/users@20091025_04h00     112M      -  30.3G  -
gescomx_pz/applications/gescomx/users@20091025_0850         0      -  30.3G  -
gescomx_pz/applications/gescomx/xchange                 8.32M  84.0G  8.28M  /applications/gescomx/xchange
gescomx_pz/applications/gescomx/xchange@20091025_04h00    44K      -  8.28M  -
gescomx_pz/applications/gescomx/xchange@20091025_0850       0      -  8.28M  -
gescomx_pz/applications/oraagent                          18K  2.00G    18K  /u01/oraagent
gescomx_pz/applications/oraagent@20091025_04h00             0      -    18K  -
gescomx_pz/applications/oraagent@20091025_0850              0      -    18K  -
gescomx_pz/zone                                         1.33G  84.0G  1.33G  /zones/gescomx_pz
gescomx_pz/zone@20091025_04h00                          3.85M      -  1.33G  -
gescomx_pz/zone@20091025_0850                               0      -  1.33G  -


##### send|recieve de POOLTOBACKUP vers NEWPOOL
LAUNCH="08:"
export LAUNCH
{
TAG="ZFS"
export TAG
-
for Filesystem in $(zfs list -Hr ${POOLTOBACKUP} | grep  $CURDATE | grep -v adadata | awk '{print $1}') 
do 
 echo "logger -p daemon.notice -t $TAG \"FS: $Filesystem\""
 echo "zfs send $Filesystem | zfs receive -dF ${NEWPOOL}"
done
echo "logger -p daemon.notice -t $TAG \"SEND RECEIVE END\""
} | at -q n $LAUNCH


##### recupere les parametres zfs de POOLTOBACKUP pour donner plus tard a NEWPOOL
{
for PARAM in mountpoint quota reservation recordsize compression  aclmode aclinherit zoned 
do
  for Filesystem in ` zfs list -Hr $POOLTOBACKUP | grep -v $CURDATE   | awk '{print $1}'`
  do
    zfs get -H $PARAM $Filesystem | perl -p -e 's:$ENV{POOLTOBACKUP}:$ENV{NEWPOOL}:'| awk '{print "zfs set "$2"="$3" "$1}'
  done
done
} | tee /var/tmp/PARAM_ZFS_$POOLTOBACKUP.txt



##### descativation de l'appli pour eviter qu'elle ne demarre au prochaine boot de la zone
zlogin gescomx_pz
/applications/gescomx/users/system/init.d/gescomx disable
svcs -a | grep gescomx
exit

##### backup
export client=opsrv169
export group=Adabas
export level=full
export fs=/applications/gescomx/adadata

savefs -s opbk01 -c ${client} -g ${group} -l ${level} ${fs}
mminfo -s opbk01  -t '1 days ago' -q 'client=${client},level=full' -r 'ssid,name,client,savetime,nfiles'


##### arret de la zone
zlogin -C gescomx_pz
init 0
~~.
zoneadm list -ivc
umount /zones/gescomx_pz

##### creation du FS manquant
zfs create ${NEWPOOL}/applications/gescomx/adadata


##### verif de la taille
root@poseidon $ zfs list -r $POOLTOBACKUP | egrep -v "${CURDATE}|@" 
NAME                                                     USED  AVAIL  REFER  MOUNTPOINT
gescomx_pz                                               131G  83.8G    18K  /zpool/gescomx_pz
gescomx_pz/applications                                  129G  83.8G    18K  none
gescomx_pz/applications/gescomx                          129G  83.8G    23K  /applications/gescomx
gescomx_pz/applications/gescomx/adadata                 69.8G  83.8G  69.8G  /applications/gescomx/adadata
gescomx_pz/applications/gescomx/adaexp                  29.1G  83.8G  29.1G  /applications/gescomx/adaexp
gescomx_pz/applications/gescomx/users                   30.6G  83.8G  30.3G  /applications/gescomx/users
gescomx_pz/applications/gescomx/xchange                 8.37M  83.8G  8.28M  /applications/gescomx/xchange
gescomx_pz/applications/oraagent                          33K  2.00G    18K  /u01/oraagent
gescomx_pz/zone                                         1.34G  83.8G  1.33G  /zones/gescomx_pz
root@poseidon $ 
root@poseidon $ 
root@poseidon $ 
root@poseidon $ zfs list -r $NEWPOOL | grep -v ${CURDATE}
NAME                                                        USED  AVAIL  REFER  MOUNTPOINT
gescomx_pz_NEW                                             60.7G   105G    21K  /gescomx_pz_NEW
gescomx_pz_NEW/applications                                59.3G   105G    21K  /gescomx_pz_NEW/applications
gescomx_pz_NEW/applications/gescomx                        59.3G   105G    23K  /gescomx_pz_NEW/applications/gescomx
gescomx_pz_NEW/applications/gescomx/adadata                  18K   105G    18K  /gescomx_pz_NEW/applications/gescomx/adadata
gescomx_pz_NEW/applications/gescomx/adaexp                 29.1G   105G  29.1G  /gescomx_pz_NEW/applications/gescomx/adaexp
gescomx_pz_NEW/applications/gescomx/users                  30.3G   105G  30.3G  /gescomx_pz_NEW/applications/gescomx/users
gescomx_pz_NEW/applications/gescomx/xchange                8.28M   105G  8.28M  /gescomx_pz_NEW/applications/gescomx/xchange
gescomx_pz_NEW/applications/oraagent                         18K   105G    18K  /gescomx_pz_NEW/applications/oraagent
gescomx_pz_NEW/zone                                        1.33G   105G  1.33G  /gescomx_pz_NEW/zone

##### changement des zpool + zone
zpool export $POOLTOBACKUP

sh /var/tmp/PARAM_ZFS_$POOLTOBACKUP.txt

zfs destroy -r ${NEWPOOL}@${CURDATE}${NEWDATE}
zfs destroy -r ${NEWPOOL}@${CURDATE}
zfs list -r ${NEWPOOL}

zpool export $NEWPOOL 
zpool import $NEWPOOL  $POOLTOBACKUP

zfs list -r $POOLTOBACKUP 


##### demarrage de la zone
zoneadm -z $POOLTOBACKUP  boot
zlogin  $POOLTOBACKUP 
svcs -x

##### verif du fs adadata
zfs list -r $POOLTOBACKUP | grep adadata

##### recherche du saveset
export client=opsrv169
export group=Adabas
export level=full
export fs=/applications/gescomx/adadata
mminfo -s opbk01 -t '1 days ago' -q 'client=opsrv169' -r 'ssid,name,client,savetime,nfiles,sscreate,savetime(20),cloneid,level,volume' | grep adadata 
3420686327 /applications/gescomx/adadata  opsrv169    10/25/09 1265 10/25/09   10/25/09 01:02:06 1256461244 full B00109
1474566417 /applications/gescomx/adadata  opsrv169    10/25/09 1264 10/25/09   10/25/09 10:23:29 1256462609 manual VTB00033
3420686327 /applications/gescomx/adadata  opsrv169    10/25/09 1265 10/25/09   10/25/09 01:02:06 1256425245 full VTB00146


export SAVESET=1474566417/1256462609

##### restauration adadata
timex recover -s opbk01 -a -iY -S ${SAVESET} -d /applications/gescomx/adadata


##### replace au bon endroit le repertoire restaure
cd /applications/gescomx/adadata/adadata
ls
mv * ..
ls
cd ..
ls
rmdir adadata/


##### On attend manu :-))))

##### nettoyage des zpool, snapshots, ...







