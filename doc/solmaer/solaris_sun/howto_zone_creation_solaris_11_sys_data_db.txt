



#############################################################################################################################
##### 
##### variables
#####




##### define variables, on all nodes

# server
export zone_name=														# the name of the new zone
export zone_alias=container													# alias of the zone for root password algorythm
export commentary=''														# a comment to specify a zone description (generally "<environment> zone for <application>")
export primary_node=														# name of the primary node of the zone
export secondary_node=														# name of the secondary node of the zone
export brand=solaris														# solaris zone type
export cluster_used=true													# if the zone will be a clustered zone (value is "true" or "false")
export physical_mem_capping=													# the capping value for physical memory
export swap_mem_capping=													# the capping value for swap
export cpu_capping=														# the capping calue for cpu

# network	
export zone_ip=															# zone IP address
export backup_client_ip=`nslookup bkp-${zone_name} | grep ^Address: | grep -v '#53' | awk '{print $2}'`				# IP address for backup client
export zone_netmask=														# in CIDR notation (/xx without "/")
export vlan_id=															# identifier of network vlan
export appli_host_name=														# the host name use by the application (generally opsvc??? or opsrv???)
export appli_host_ip=$zone_ip													# the IP address of $appli_host_name (by default this is the same of zone, because host name is a DNS canonical name of zone)
export network_interface=aggr1													# network interface use by zone (lower-link option for zonecfg command)
export default_router=														# the network default route for the zon

# application	
export appli_name=														# the name of the application (will define /applications/${appli_name}
export appli_project=$appli_name												# system project name (by default, match with the application name)
export appli_project_id=													# system project id
export appli_user=$appli_name													# application user name (by default, match with the application name)
export appli_uid=														# application user id
export comment_appli_user="${appli_user} user for ${appli_project} project"							# a comment to specify a application description
export w_appli_user="w_${appli_name}"												# specific user for wood
export w_appli_uid=														# wood user id
export comment_w_appli_user="${w_appli_user} wood user for ${appli_project} project"						# a comment to specify the application wood description
export appli_group=$appli_name													# application group name (by default, match with the application name)
export appli_gid=														# application group id
export test_used=														# if the zone will be accessed by test integration team (value is "true" or "false")
export oracle_used=														# if the zone will use oracle database (value is "true" or "false")
export wood_used=														# if the zone will use wood (value is "true" or "false")
export asm_used=														# if the zone will use oracle ASM (value is "true" or "false")
export documentum_used=														# if the zone will use documentum (value is "true" or "false")

# storage
export powerpath_used=true
export srdf_used=true
export zpool_replicat_type=srdf															# "srdf", "none" to define the type of replicat within zpools
export rdf_device_groupe_name=${zone_name}													# the rdf device group name for LUNs for system zpool

export hex_lun_id_sys_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "35 36 37")
export dec_lun_id_sys_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "7 8 9")
export rdf_dev_list_sys=''															# provided by storage team, if many devices please separate it with space character - Example : "220B 2245")

export hex_lun_id_db_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "35 36 37")
export dec_lun_id_db_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "7 8 9")
export rdf_dev_list_db=''															# provided by storage team, if many devices please separate it with space character - Example : "220B 2245")

export hex_lun_id_data_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "35 36 37")
export dec_lun_id_data_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "7 8 9")
export rdf_dev_list_data=''															# provided by storage team, if many devices please separate it with space character - Example : "220B 2245")

export hex_lun_id_asm_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "35 36 37")
export dec_lun_id_asm_list='' 															# provided by storage team, if many devices please separate it with space character - Example : "7 8 9")
export rdf_dev_list_asm=''															# provided by storage team, if many devices please separate it with space character - Example : "220B 2245")

export local_site=$(/home/admin/bin/getcmdb.sh host | grep `uname -n` | awk -F';' '{print $5}' | awk '{print $1}')				# provide the primary site, based on the CMDB
export remote_site=$(/home/admin/bin/getcmdb.sh host | grep $(clnode list| grep -v `uname -n`)  | awk -F';' '{print $5}' | awk '{print $1}')	# provide the primary site, based on the CMDB
export mercier_storage=																# SymmID possible: 000292603453 000296700060 
export eufo_storage=																# SymmID possible: 000292602560 000296700069
if [[ $local_site == 'MER' ]]; then export local_storage=$mercier_storage;  fi
if [[ $local_site == 'EUFO' ]]; then export local_storage=$eufo_storage;  fi
export storage_info_file=/home/betorma/docs/storage.txt


# other
export tmp_dir=/home/systemstore/temp/${zone_name}     												# folder used to store temporay files or configuration files
export puppet_git_repo=/home/betorma/git/puppet-pk/development											# folder used as local git repository for puppet
mkdir -p $tmp_dir
export date=`date +%Y%m%d%H%M`	
export int_test="`getent group int_test | awk -F':' '{print $4}' | sed -e 's/,/ /g'`"								# test integrator members



#############################################################################################################################
##### 
##### lun check
#####



##### on all nodes

cfgadm -al
powermt check
powermt display dev=all class=all
symcfg discover


##### on one node

cldev populate



##### on all nodes

/home/betorma/bin/storage_info.pl -A >${tmp_dir}/storage_info_`uname -n`.txt


##### if hexa lun id are know, search luns with hexa lun id, else use decimal lun id

{
if [[ -n $hex_lun_id_sys_list ]]; then
	for dev in $hex_lun_id_sys_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
else
	for dev in $dec_lun_id_sys_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
fi
if [[ -n $hex_lun_id_data_list ]]; then
	for dev in $hex_lun_id_data_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
else
	for dev in $dec_lun_id_data_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
fi
if [[ x${oracle_used} == xtrue ]]; then
	if [[ -n $hex_lun_id_db_list ]]; then
		for dev in $hex_lun_id_db_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
	else
		for dev in $dec_lun_id_db_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
	fi
fi
if [[ x${asm_used} == xtrue ]]; then
	if [[ -n $hex_lun_id_asm_list ]]; then
		for dev in $hex_lun_id_asm_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
	else
		for dev in $dec_lun_id_asm_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt; done
	fi
fi
} | grep -i vmax3



#############################################################################################################################
##### 
##### rdf device groups creation
#####
#####



##### check if we are in client/server mode for EMC Solution Enabler

{
env | grep ^SYMCLI >/dev/null
if [[ ! $? = 0 ]]; then
	echo; echo "ERROR: This server don't use EMC Solution Enabler with client/server/mode"
fi
}



##### define/check rdf type, on each node

{
if [ $primary_node = `uname -n` ]; then
	rdf_type=RDF1
else 
	rdf_type=RDF2
fi
if [ -z $rdf_type ]; then
	echo "Can't determine primary node and RDF type for this node."
	echo 'Please check or define RDF type manually.'
else
	echo $rdf_type
fi
}



##### check if devices are with the good rdf type

{
for dev in $rdf_dev_list_sys $rdf_dev_list_db $rdf_dev_list_data $rdf_dev_list_asm
do
	echo $dev
	if [ $local_site = 'EUFO' ]; then
		symdev show -sid $eufo_storage $dev | grep 'Device Configuration' | grep $rdf_type >/dev/null
		if [ $? != 0 ]; then
			echo "$dev is not with the good rdf type."
		fi
	fi
	if [ $local_site = 'MER' ]; then
		symdev show -sid $mercier_storage $dev | grep 'Device Configuration' | grep $rdf_type >/dev/null
		if [ $? != 0 ]; then
			echo "$dev is not with the good rdf type."
		fi
	fi
done
}



##### create rdf device group if it don't exists, on all nodes

{
symdg show ${rdf_device_groupe_name} >/dev/null
if [[ $? != 0 ]]; then 
	symdg -type $rdf_type create $rdf_device_groupe_name
	if [[ $? == 0 ]];then echo $rdf_device_groupe_name created.; fi
else
	echo $rdf_device_groupe_name already exists
fi
}



##### add devices within rdf device groups

{
for dev in $rdf_dev_list_sys; do symdg -g $rdf_device_groupe_name -sid $local_storage add dev $dev; done
for dev in $rdf_dev_list_data; do symdg -g $rdf_device_groupe_name -sid $local_storage add dev $dev; done
if [[ x${oracle_used} == xtrue ]]; then for dev in $rdf_dev_list_db; do symdg -g $rdf_device_groupe_name -sid $local_storage add dev $dev; done; fi
if [[ x${asm_used} == xtrue ]]; then for dev in $rdf_dev_list_asm; do symdg -g $rdf_device_groupe_name -sid $local_storage add dev $dev; done; fi
}



##### check rdf sync, on primary node

symrdf query -g $rdf_device_groupe_name | grep '^DEV' | grep -v 'Synchronized$'



#############################################################################################################################
##### 
##### combine cluster device group for system
#####
#####


##### get concerned cluster devices, on each node

{
for dev in $rdf_dev_list_sys
do
	grep -i " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | perl -ne 'print "$1\n" if(m{#\s(d\d+)\s#})'
done
} | sort -u >${tmp_dir}/did_sys.${rdf_type}
cat ${tmp_dir}/did_sys.${rdf_type}


##### label disk on primary node

{
for dev in $rdf_dev_list_sys
do
	grep -i " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | tail -1 | perl -ne '@list=split; print "\n{echo label; echo yes} | format -d $list[2]\n"'
done
}



##### combine cluster devices, on secondary/rdf2 node

{
export device_number=`wc -l ${tmp_dir}/did_sys.${rdf_type} | awk '{print $1}'`
if [[ $rdf_type == 'RDF2' ]]; then 

	for i in $(seq 1 `echo $device_number`)
	do
		echo cldev combine -t srdf -g ${rdf_device_groupe_name} -d `cat ${tmp_dir}/did_sys.RDF1 | head -${i} | tail -1` `cat ${tmp_dir}/did_sys.RDF2 | head -${i} | tail -1`
		echo didadm -F scsi3 `cat ${tmp_dir}/did_sys.RDF1 | head -${i} | tail -1`
		echo cldev show `cat ${tmp_dir}/did_sys.RDF1 | head -${i} | tail -1`
		echo
	done
else
	echo; echo 'ERROR: you are not on RDF2 node to combine cluster devices.'
fi
}



{
cat ${tmp_dir}/did_sys.RDF[12] | while read did
do
	echo cldg offline dsk/${did}
	echo cldg disable dsk/${did}
	echo cldg delete dsk/${did}
done
}



#############################################################################################################################
##### 
##### combine cluster device group for data
#####



##### get concerned cluster devices, on each node

{
for dev in $rdf_dev_list_data
do
	grep " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | perl -ne 'print "$1\n" if(m{#\s(d\d+)\s#})'
done
} | sort -u >${tmp_dir}/did_data.${rdf_type}
cat ${tmp_dir}/did_data.${rdf_type}



##### label disk on primary node

{
for dev in $rdf_dev_list_data
do
	grep -i " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | tail -1 | perl -ne '@list=split; print "\n{echo label; echo yes} | format -d $list[2]\n"'
done
}



##### combine cluster devices, on secondary/rdf2 node

{
export device_number=`wc -l ${tmp_dir}/did_data.${rdf_type} | awk '{print $1}'`
if [[ $rdf_type == 'RDF2' ]]; then 

	for i in $(seq 1 `echo $device_number`)
	do
		echo cldev combine -t srdf -g ${rdf_device_groupe_name} -d `cat ${tmp_dir}/did_data.RDF1 | head -${i} | tail -1` `cat ${tmp_dir}/did_data.RDF2 | head -${i} | tail -1`
		echo didadm -F scsi3 `cat ${tmp_dir}/did_data.RDF1 | head -${i} | tail -1`
		echo cldev show `cat ${tmp_dir}/did_data.RDF1 | head -${i} | tail -1`
		echo
	done
else
	echo; echo 'ERROR: you are not on RDF2 node to combine cluster devices.'
fi
}



{
cat ${tmp_dir}/did_data.RDF[12] | while read did
do
	echo cldg offline dsk/${did}
	echo cldg disable dsk/${did}
	echo cldg delete dsk/${did}
done
}



#############################################################################################################################
##### 
##### combine cluster device group for db
#####



##### get concerned cluster devices, on each node

{
for dev in $rdf_dev_list_db
do
	grep " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | perl -ne 'print "$1\n" if(m{#\s(d\d+)\s#})'
done
} | sort -u >${tmp_dir}/did_db.${rdf_type}
cat ${tmp_dir}/did_db.${rdf_type}



##### label disk on primary node

{
for dev in $rdf_dev_list_db
do
	grep -i " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | tail -1 | perl -ne '@list=split; print "\n{echo label; echo yes} | format -d $list[2]\n"'
done
}



##### combine cluster devices, on secondary/rdf2 node

{
export device_number=`wc -l ${tmp_dir}/did_db.${rdf_type} | awk '{print $1}'`
if [[ $rdf_type == 'RDF2' ]]; then 

	for i in $(seq 1 `echo $device_number`)
	do
		echo cldev combine -t srdf -g ${rdf_device_groupe_name} -d `cat ${tmp_dir}/did_db.RDF1 | head -${i} | tail -1` `cat ${tmp_dir}/did_db.RDF2 | head -${i} | tail -1`
		echo didadm -F scsi3 `cat ${tmp_dir}/did_db.RDF1 | head -${i} | tail -1`
		echo cldev show `cat ${tmp_dir}/did_db.RDF1 | head -${i} | tail -1`
		echo
	done
else
	echo; echo 'ERROR: you are not on RDF2 node to combine cluster devices.'
fi
}



{
cat ${tmp_dir}/did_db.RDF[12] | while read did
do
	echo cldg offline dsk/${did}
	echo cldg disable dsk/${did}
	echo cldg delete dsk/${did}
done
}



#############################################################################################################################
##### 
##### cluster device groups creation for asm 
#####
##### if SRDF replication is not used, this paragraph is not necessary
#####



##### get concerned cluster devices, on each node

{
for dev in $rdf_dev_list_asm
do
	grep " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | perl -ne 'print "$1\n" if(m{#\s(d\d+)\s#})'
done
} | sort -u >${tmp_dir}/did_asm.${rdf_type}
cat ${tmp_dir}/did_asm.${rdf_type}



##### label disk

{
for dev in $rdf_dev_list_asm
do
	grep -i " $dev #" ${tmp_dir}/storage_info_`uname -n`.txt | tail -1 | perl -ne '@list=split; print "\n{echo label; echo yes} | format -d $list[2]\n"'
done
}



##### combine cluster devices, on secondary/rdf2 node

{
export device_number=`wc -l ${tmp_dir}/did_asm.${rdf_type} | awk '{print $1}'`
if [[ $rdf_type == 'RDF2' ]]; then 

	for i in $(seq 1 `echo $device_number`)
	do
		echo cldev combine -t srdf -g ${rdf_device_groupe_name} -d `cat ${tmp_dir}/did_asm.RDF1 | head -${i} | tail -1` `cat ${tmp_dir}/did_asm.RDF2 | head -${i} | tail -1`
		echo didadm -F scsi3 `cat ${tmp_dir}/did_asm.RDF1 | head -${i} | tail -1`
		echo cldev show `cat ${tmp_dir}/did_asm.RDF1 | head -${i} | tail -1`
		echo
	done
else
	echo; echo 'ERROR: you are not on RDF2 node to combine cluster devices.'
fi
}



{
cat ${tmp_dir}/did_asm.RDF[12] | while read did
do
	echo cldg offline dsk/${did}
	echo cldg disable dsk/${did}
	echo cldg delete dsk/${did}
done
}



#############################################################################################################################
##### 
##### cluster device group creation
#####

if [[ $rdf_type == 'RDF1' ]]; then 
	#cldg create -n `clnode list | xargs | sed -e 's/ /,/'` -t rawdisk -d `cat ${tmp_dir}/did_sys.RDF1 ${tmp_dir}/did_data.RDF1 ${tmp_dir}/did_db.RDF1 ${tmp_dir}/did_asm.RDF1 2>/dev/null | xargs | sed -e 's/ /,/g'` ${rdf_device_groupe_name}
	cldg create -n ${primary_node},${secondary_node} -t rawdisk -d `cat ${tmp_dir}/did_sys.RDF1 ${tmp_dir}/did_data.RDF1 ${tmp_dir}/did_db.RDF1 ${tmp_dir}/did_asm.RDF1 2>/dev/null | xargs | sed -e 's/ /,/g'` ${rdf_device_groupe_name}
	cldg show -v ${rdf_device_groupe_name}
	cldg online ${rdf_device_groupe_name}
	cldg switch -n ${primary_node} ${rdf_device_groupe_name}
	cldg status ${rdf_device_groupe_name}
else
	echo; echo 'ERROR: you are not on RDF1 node to create cluster device goup.'
fi




#############################################################################################################################
##### 
##### asm format
#####
##### if ASM is not used, this paragraph is not necessary
#####



{
if [[ $asm_use == 'true' ]]; then
	if [[ $rdf_type == 'RDF1' ]]; then 
		cat ${tmp_dir}/did_asm.RDF1 | while read did
		do
			cldev list -v $did | grep `uname -n` |tail -1
		done | awk -F'/' '{print "/home/betorma/bin/op_format_s0.sh --asm "$4}' 
	else
		echo; echo 'ERROR: you are not on RDF1 node to format disks.'
	fi
fi
}



#############################################################################################################################
##### 
##### zpool creation
#####



##### format disk to align blocks with EMC blocks, on primary node

{
if [[ -n $hex_lun_id_sys_list ]]; then
	for dev in $hex_lun_id_sys_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
else
	for dev in $dec_lun_id_sys_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
fi
if [[ -n $hex_lun_id_data_list ]]; then
	for dev in $hex_lun_id_data_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
else
	for dev in $dec_lun_id_data_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
fi
if [[ x${oracle_used} == xtrue ]]; then
	if [[ -n $hex_lun_id_db_list ]]; then
		for dev in $hex_lun_id_db_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
	else
		for dev in $dec_lun_id_db_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
	fi
fi
if [[ x${asm_used} == xtrue ]]; then
	if [[ -n $hex_lun_id_asm_list ]]; then
		for dev in $hex_lun_id_asm_list; do grep -i " 0x${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
	else
		for dev in $dec_lun_id_asm_list; do grep -i " ${dev} " ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | perl -ne 'print "/home/betorma/bin/op_format_s0.sh --emc $1\n" if(m{\s+(emcpower\d+.)\s+})' | sort -u ; done 
	fi
fi
}



##### zpool creation, on primary node

{
if [[ -n $hex_lun_id_sys_list ]]; then
	primary_sys_mirror_dev=$(for dev in $hex_lun_id_sys_list; do grep -i ^$local_site $storage_info_file | awk '{print $3}' | while read wwn; do grep $wwn ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | grep " 0x${dev} "; done | perl -ne 'print "$1\n" if(m	{ (emcpower\d+a) })' | sort -u | sed -e 's/a$/c/'; done)
else
	primary_sys_mirror_dev=$(for dev in $dec_lun_id_sys_list; do grep -i ^$local_site $storage_info_file | awk '{print $3}' | while read wwn; do grep $wwn ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | grep " $dev "; done | perl -ne 'print "$1\n" if(m{ (emcpower\d+a) })' | sort -u | sed -e 's/a$/c/'; done)
fi
if [[ -n $hex_lun_id_data_list ]]; then
	primary_data_mirror_dev=$(for dev in $hex_lun_id_data_list; do grep -i ^$local_site $storage_info_file | awk '{print $3}' | while read wwn; do grep $wwn ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | grep " 0x${dev} "; done | perl -ne 'print "$1\n" if(m	{ (emcpower\d+a) })' | sort -u | sed -e 's/a$/c/'; done)
else
	primary_data_mirror_dev=$(for dev in $dec_lun_id_data_list; do grep -i ^$local_site $storage_info_file | awk '{print $3}' | while read wwn; do grep $wwn ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | grep " $dev "; done | perl -ne 'print "$1\n" if(m{ (emcpower\d+a) })' | sort -u | sed -e 's/a$/c/'; done)
fi
if [[ -n $hex_lun_id_db_list ]]; then
	primary_db_mirror_dev=$(for dev in $hex_lun_id_db_list; do grep -i ^$local_site $storage_info_file | awk '{print $3}' | while read wwn; do grep $wwn ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | grep " 0x${dev} "; done | perl -ne 'print "$1\n" if(m	{ (emcpower\d+a) })' | sort -u | sed -e 's/a$/c/'; done)
else
	primary_db_mirror_dev=$(for dev in $dec_lun_id_db_list; do grep -i ^$local_site $storage_info_file | awk '{print $3}' | while read wwn; do grep $wwn ${tmp_dir}/storage_info_`uname -n`.txt | grep -i vmax3 | grep " $dev "; done | perl -ne 'print "$1\n" if(m{ (emcpower\d+a) })' | sort -u | sed -e 's/a$/c/'; done)
fi

echo zpool create ${zone_name}-sys $primary_sys_mirror_dev
echo zpool create ${zone_name}-data $primary_data_mirror_dev
echo zpool create ${zone_name}-db $primary_db_mirror_dev
}





##### default zfs filesystems creation for zone and application system

{
zfs set mountpoint=/zpool/${zone_name}-sys ${zone_name}-sys
zfs create -o mountpoint=/zones/${zone_name} ${zone_name}-sys/zone
zfs create -o mountpoint=none -o zoned=on ${zone_name}-sys/applications

for FS in orabin users; do
	zfs create -p -o mountpoint=/applications/${appli_name}/${FS} ${zone_name}-sys/applications/${appli_name}/${FS}
	zfs set recordsize=128K ${zone_name}-sys/applications/${appli_name}/${FS}
	zfs set logbias=latency ${zone_name}-sys/applications/${appli_name}/${FS}
done

zfs list -r ${zone_name}-sys
}



##### default zfs filesystems creation for data application

{
zfs create -o mountpoint=none -o zoned=on ${zone_name}-data/applications
for FS in xchange ; do
	zfs create -p -o mountpoint=/applications/${appli_name}/${FS} ${zone_name}-data/applications/${appli_name}/${FS}
	zfs set recordsize=128K ${zone_name}-data/applications/${appli_name}/${FS}
	zfs set logbias=latency ${zone_name}-data/applications/${appli_name}/${FS}
done
zfs list -r ${zone_name}-data
}


###### if documentum is used

{
if [[ x${documentum_used} == xtrue ]]; then
	for FS in docdata; do
		zfs create -o mountpoint=/applications/${appli_name}/${FS} ${zone_name}-data/applications/${appli_name}/${FS}
		zfs set recordsize=128K ${zone_name}-data/applications/${appli_name}/${FS}
		zfs set logbias=latency ${zone_name}-data/applications/${appli_name}/${FS}
	done
	zfs list -r ${zone_name}-data
fi
}



##### default zfs filesystems creation for application database

{
if [[ x${oracle_used} == xtrue ]]; then
	zfs set mountpoint=/zpool/${zone_name}-db ${zone_name}-db
	zfs create -o mountpoint=none -o zoned=on ${zone_name}-db/applications
	zfs create -o mountpoint=none ${zone_name}-db/applications/${appli_name}

	for FS in oralog oradata oraflash oraonlinelog; do
		zfs create -p -o mountpoint=/applications/${appli_name}/${FS} ${zone_name}-db/applications/${appli_name}/${FS}
	done

	zfs set recordsize=8K ${zone_name}-db/applications/${appli_name}/oradata
	zfs set logbias=latency ${zone_name}-db/applications/${appli_name}/oradata
	for FS in oralog oraflash oraonlinelog ; do
		zfs set recordsize=128K ${zone_name}-db/applications/${appli_name}/${FS}
		zfs set logbias=latency ${zone_name}-db/applications/${appli_name}/${FS}
	done

	zfs list -r ${zone_name}-db
fi
}



#############################################################################################################################
##### 
##### zone creation and configuration
##### 



##### creation, on primary node

{
zonecfg -z ${zone_name} <<EOF
create -F -b
set zonepath=/zones/${zone_name}
set autoboot=false
set bootargs="-m verbose"

set ip-type=exclusive

add anet
set linkname=net0_${vlan_id}
set allowed-address="${zone_ip}/${zone_netmask}"
set lower-link=${network_interface}
set defrouter=${default_router}
set vlan-id=${vlan_id}
set mac-address=random
set configure-allowed-address=true
end

add anet
set linkname=net0_167
set allowed-address="${backup_client_ip}/16"
set lower-link=${network_interface}
set vlan-id=167
set mac-address=random
set configure-allowed-address=true
end

add attr
set name=comment
set type=string
set value="zone"
end

add attr
set name=osc-ha-zone
set type=boolean
set value=true
end

add dataset
set name=${zone_name}-sys/applications
set alias=${zone_name}-sys_applications
end

add dataset
set name=${zone_name}-data/applications
set alias=${zone_name}-data_applications
end

add dataset
set name=${zone_name}-db/applications
set alias=${zone_name}-db_applications
end

commit
exit
EOF
}




##### capping

{
zonecfg -z $zone_name <<EOT
add capped-memory
set physical=${physical_mem_capping}
set swap=${swap_mem_capping}
end
commit
exit
EOT
}

{
zonecfg -z $zone_name <<EOT
add capped-cpu
set ncpus=${cpu_capping}
end
commit
exit
EOT
}

{
zonecfg -z $zone_name info capped-cpu
zonecfg -z $zone_name info capped-memory
}



##### installation

zoneadm -z ${zone_name} install -c /home/admin/unix/opoce-zone-sysconfig.xml -m /home/admin/unix/opoce-zone-manifest.xml




##### /etc/inet/hosts

{
if [[ $zone_ip == $appli_host_ip ]]; then
	export inet_host="$zone_ip $zone_name # $appli_host_name"
else
	export inet_host="$zone_ip $zone_name\n$appli_host_ip $appli_host_name"
fi
}

cat <<EOF >/zones/${zone_name}/root/etc/inet/hosts
::1 localhost
127.0.0.1 localhost
$inet_host
EOF



##### root as user

cp /etc/security/exec_attr.d/OProot /zones/${zone_name}/root/etc/security/exec_attr.d
cp /etc/security/prof_attr.d/OProot /zones/${zone_name}/root/etc/security/prof_attr.d



##### boot and connection

zoneadm -z ${zone_name} boot && zlogin -C ${zone_name}
~~.



##### hostname

zlogin ${zone_name} /usr/bin/hostname ${zone_name}



##### puppet agent

cp /home/systemstore/puppet/puppet.xml /zones/${zone_name}/root/tmp/
zlogin ${zone_name} /usr/sbin/svccfg import /tmp/puppet.xml




#############################################################
# 
# on puppet server
#

puppet cert --list
puppet cert --list | awk -F'"' '{print "puppet cert sign "$2}' 



#############################################################
# 
# on the primary node
#

##### puppet agent

zlogin ${zone_name}
puppet agent -t



#############################################################
# 
# on puppet server
# first within your own local git repository
# then push the modifications to puppet server
#

##### create/import a new class for the application

{
if [[ -f ${puppet_git_repo}/modules/applications/manifests/${appli_name}.pp ]];then
	echo "${puppet_git_repo}/modules/applications/manifests/${appli_name}.pp already exist, please check or populate it manually."
else
	cat >${puppet_git_repo}/modules/applications/manifests/${appli_name}.pp <<EOF
class applications::${appli_name} {

  case \$operatingsystem {

    Solaris: {

      \$application_name = "${appli_name}"
      \$oracle_database_is_used = ${oracle_used}
      \$documentum_is_used = ${documentum_used}
      \$wood_is_used = ${wood_used}
      \$user_id = "${appli_uid}"
      \$w_user_id = "${w_appli_uid}"
      \$group_id = "${appli_gid}"
      \$project_id = "${appli_project_id}"
      \$user_name = \$application_name
      \$zfs_sys_pool_name = "\${hostname}-sys"
      \$zfs_data_pool_name = "\${hostname}-data"
      \$zfs_db_pool_name = "\${hostname}-db"
      \$user_comment = "\${user_name} user for \${application_name} project"
      \$w_user_name = "w_\${user_name}"
      \$w_user_comment = "\${w_user_name} wood user for \${application_name} project"
      \$group_name = \$application_name

      generic_new { "$application_name":
        application_name => \$application_name,
        oracle_database_is_used => \$oracle_database_is_used,
        documentum_is_used => \$documentum_is_used,
        wood_is_used => \$wood_is_used,
        user_id => \$user_id,
        w_user_id => \$w_user_id,
        group_id => \$group_id,
        project_id => \$project_id,
        user_name => \$user_name,
        zfs_sys_pool_name => \$zfs_sys_pool_name,
        zfs_data_pool_name => \$zfs_data_pool_name,
        zfs_db_pool_name => \$zfs_db_pool_name,
        user_comment => \$user_comment,
        w_user_name => \$w_user_name,
        w_user_comment => \$w_user_comment,
        group_name => \$group_name,
      }

    } # end case Solaris

  } # end case $operatingsystem

} # end class
EOF
fi
}


{
if [[ $test_used == 'true' ]]; then
	if [[ -f ${puppet_git_repo}/modules/applications/files/Solaris/etc/user_attr.d/OPinttest.${appli_name} ]];then
		echo "${puppet_git_repo}/modules/applications/files/Solaris/etc/user_attr.d/OPinttest.${appli_name} already exist, please check or populate it manually."
	else
		cat >${puppet_git_repo}/modules/applications/files/Solaris/etc/user_attr.d/OPinttest.${appli_name} <<EOF
#
# deployed by puppet
#

`for user in $int_test
do
	if [[ $wood_used == 'true' ]]; then echo "${user}::::type=normal;roles=${appli_name},w_${appli_name}"; fi
	if [[ $wood_used == 'false' ]]; then echo "${user}::::type=normal;roles=${appli_name}"; fi
done`
EOF
	fi
fi
}



{
echo
echo To execute within your GIT environment:
echo
echo git pull
echo cd $puppet_git_repo
echo git add ${puppet_git_repo}/modules/applications/manifests/${appli_name}.pp 
if [[ $test_used == 'true' ]]; then echo git add ${puppet_git_repo}/modules/applications/files/Solaris/etc/user_attr.d/OPinttest.${appli_name}; fi
echo git commit -m \"add new ${appli_name} application\"
echo git push
echo cd $puppet_git_repo/../production
echo git pull origin production \&\& git pull --no-ff
echo
echo git push \&\& cd \-
}



#############################################################
# 
# on foreman web interface
#

# import new puppet class
# configure host



#############################################################
# 
# on primary node
#

##### puppet agent

zlogin ${zone_name} puppet agent -t



#############################################################
# 
# on zone
#


##### change root password to match with algorythm

passwd root


##### change application users password 

passwd ${appli_user}
passwd ${w_appli_user}



##### restart and check

init 6 
svcs -xv
dmesg




#############################################################################################################################
##### 
##### asm device assignment to the zone
#####


{
if [[ $rdf_type == 'RDF1' ]]; then 
	echo "zonecfg -z ${zone_name} <<EOT" 
	cat ${tmp_dir}/did_asm.RDF1 | while read did
	do
		echo "add device"
		echo "set match=/dev/did/*dsk/${did}s*"
		echo "end"	
	done

echo verify
echo commit
echo exit
echo EOT
else
	echo; echo 'ERROR: you are not on RDF1 node to format disks.'
fi
}

zoneadm -z $zone_name apply
zlogin $zone_name "chown oracle:dba /dev/did/{dsk,rdsk}/*"



#############################################################################################################################
##### 
##### cluster integration
##### 



##### check resource type registration

/usr/cluster/bin/clresourcetype  list SUNW.HAStoragePlus || clresourcetype register SUNW.HAStoragePlus
/usr/cluster/bin/clresourcetype  list SUNW.gds || clresourcetype register SUNW.gds



##### stop and detach zone

zlogin $zone_name init 0 && zlogin -C $zone_name 
~~.
zoneadm -z $zone_name detach
grep $zone_name /etc/zones/index > ${tmp_dir}/index


##### export zpools

zpool export ${zone_name}-data
zpool export ${zone_name}-sys
zpool export ${zone_name}-db



##### check/swicth on primary node the cluster device group
cldg switch -n ${primary_node} ${zone_name}



##### create resources for rdf disk group and zpool

clrg create ${zone_name}-rg
clrg manage ${zone_name}-rg
clrg set -p Nodelist=${primary_node},${secondary_node} ${zone_name}-rg



echo clrs create -g ${zone_name}-rg -t SUNW.HAStoragePlus -p GlobalDevicePaths="${zone_name}" ${zone_name}-srdf
echo clrs create -g ${zone_name}-rg -t SUNW.HAStoragePlus -p zpools="${zone_name}-data,${zone_name}-sys,${zone_name}-db" -p Resource_dependencies="${zone_name}-srdf" ${zone_name}-zfs



##### create resource for zone, on both nodes

cat <<- EOT >/opt/SUNWsczone/sczbt/util/sczbt_${zone_name}-rs
RS=${zone_name}-rs
RG=${zone_name}-rg
PARAMETERDIR=/etc/zones
SC_NETWORK=false
SC_LH=
FAILOVER=true
HAS_RS=${zone_name}-zfs
Zonename="${zone_name}"
Zonebrand="solaris"
Zonebootopt=""
Milestone="multi-user-server"
Migrationtype="cold"
LXrunlevel="3"
SLrunlevel="3"
Mounts=""
EOT



##### on primary node

clrg online ${zone_name}-rg
clrg status ${zone_name}-rg

cp /etc/zones/${zone_name}.xml ${tmp_dir}



##### on secondary node

cp ${tmp_dir}/${zone_name}.xml /etc/zones
cat ${tmp_dir}/index >>/etc/zones/index



##### on primary node, create and enable resource for zone

/opt/SUNWsczone/sczbt/util/sczbt_register -f /opt/SUNWsczone/sczbt/util/sczbt_${zone_name}-rs
clrs enable ${zone_name}-rs
clrs status ${zone_name}-rs
clrs unmonitor ${zone_name}-rs
clrs status ${zone_name}-rs



##### switch test

timex clrg switch -n ${secondary_node} ${zone_name}-rg
clrs status -g ${zone_name}-rg
timex clrg switch -n ${primary_node} ${zone_name}-rg
clrs status -g ${zone_name}-rg



#############################################################################################################################
##### 
##### cold backup client
##### 

cat <<EOF >${puppet_git_repo}/modules/solaris/files/Solaris/nsr/scripts/snapshot.ksh.conf.${zone_name}
cat <<EOF >/tmp/test
#
#       $Id: ${zone_name}.snapshot.ksh.conf,v 1.1 2014/08/06 12:32:03 cherrol Exp $
#
#
# Set it to 1 if you want debug output by e-mail.
DEBUG=0
#
#
# The email destination, if wanted.
DEBUG_DST='OP-INFRA-OPENSYSTEM-LOGS@publications.europa.eu>'
#
#
# The ZFS pool names.
set -A ZPOOLS ${zone_name}-sys_applications ${zone_name}-data_applications ${zone_name}-db_applications
#
#
# The applications to stop/start.
set -A APPLIS_STOP ${appli_name} 
#
#
# The Nagios targets where we should stop (and start) the probbing.
set -A NAGIOS_TARGETS ${appli_host_name}
#
#
# The Nagios user to use.
NAGIOSUSER='backupUsr'
#
#
# The Nagios password to use.
NAGIOSPASSWD='N3tw0rk3R'
#
#
# If TEST_MODE is defined (usually to 'echo'), use it as a prefix
# for stop / start commands.
#TEST_MODE='echo'
EOF

##### push to git production repository



#############################################################################################################################
##### 
##### installation de solution enabler
##### 

##### install
#
###### envoyer unique ID
#
#

unset SYMCLI_CONNECT SYMCLI_CONNECT_TYPE
symacl -unique                                               



#############################################################################################################################
##### 
##### cmdb
##### 

##### on the primary node

{
echo "nom de la zone:\t\t\t\t${zone_name} (${zone_ip})"
echo "OS version:\t\t\t\t"`head -1 /zones/${zone_name}/root/etc/release`
echo "hote de l'application:\t\t\t${appli_host_name} (${appli_host_ip})"
echo "vlan configured:\t\t\t"`zonecfg -z ${zone_name} info | grep vlan-id | awk '{print $2}'`
echo "alias de la zone:\t\t\t${zone_alias}"
echo
echo "capping cpu:\t\t\t`zonecfg -z ${zone_name} info capped-cpu | grep ncpus: | awk '{print $2}' | sed -e 's/\]//g'`"
echo "capping memoire physique:\t`zonecfg -z ${zone_name} info capped-memory | grep physical: | awk '{print $2}'`"
echo "capping swap:\t\t\t`zonecfg -z ${zone_name} info capped-memory | grep swap: | awk '{print $2}' | sed -e 's/\]//g'`"
echo
echo "file systems:"
zonecfg -z ${zone_name} info | perl -n0777e 'while(m{dataset:\s+name:\s+(.*?)/.*?\s+}g) {print "$1 "}' | xargs zfs list -r -o name,mountpoint
echo
if [[ ${cluster_used} == true ]]
then
	clrg show -p nodelist  ${zone_name}-rg | grep 'Nodelist:' | perl -ne 'if(m{Nodelist:\s+(.*?)\s+(.*?)\s+}) {print "primary cluster node:\t\t$1\nsecondary cluster node:\t\t$2\n"}'
else
        echo "hote physique:\t\t\t`uname -n`"
fi
} | mailx -s "mise a jour de la cmdb: ${zone_name}" -r mathieu.betori@ext.publications.europa.eu -c mathieu.betori@ext.publications.europa.eu OP-INFRA-OPENSYSTEMS-CHGMGT@publications.europa.eu



#############################################################################################################################
##### 
##### monitoring
##### 




{
cat <<EOT
Pouvez-vous s'il vous plait créer le monitoring pour les hotes suivants;
 - $zone_name
 - $appli_host_name
Merci d'avance
EOT
} | mailx -s "ouverture de ticket: monitoring creation for $zone_name" -r mathieu.betori@ext.publications.europa.eu -c 'mathieu.betori@ext.publications.europa.eu op-helpdesk@publications.europa.eu' OP-IT-PRODUCTION@publications.europa.eu




#############################################################################################################################
##### 
##### backup
##### 


##### create the res file (if the client is already configured)

networker_group=`{
nsradmin -s opvmwsbkp06 -i - <<EOT
show group
print name: bkp-${zone_name}
EOT
}` 


{
echo $networker_group | grep 'No resources found for query:' >/dev/null
if [[ $? != 0 ]]; then 
	group_name=`echo $networker_group | awk '{print $2}' | sed -e 's/;$//'`
	cat <<EOF >/nsr/res/${group_name}.res
type: savepnpc;
precmd: "/nsr/scripts/snapshot.ksh precmd";
pstcmd: "/nsr/scripts/snapshot.ksh pstcmd";
abort precmd with group: No;
EOF
else
	request="and communicate to us his networker group name "
fi
}


##### preciser le site du primary node

{
cat <<EOT
Hi,

Can you please create bkp-${zone_name} backup client for ${zone_name} zone $request?
His primary node is ${primary_node} (at `echo $local_site | sed -e 's/MER/mercier/' -e 's/EUFO/eufo/'`).

Thanks in advance.

EOT
} | mailx -s "backup client creation for $zone_name" -r mathieu.betori@ext.publications.europa.eu -c mathieu.betori@ext.publications.europa.eu OPDL-A4-STORAGE-BACKUP@publications.europa.eu




#############################################################################################################################
##### 
##### inform integration an db teams that the environment is ready


{
cat <<EOT
Hi,

$zone_name is ready and available.

EOT
} | mailx -s "$zone_name" betorma











