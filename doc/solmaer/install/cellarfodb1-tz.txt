##############################################################################################################################################
# howto create solaris 11 clustered zone with zfs on srdf 
##############################################################################################################################################1110



#######################################################################
##### storage provisionning


 	Size	Lun	 	Master Device	 	 	Replica Device
Device	GB	Type	Dec	Hexa	Host	LUN ID	Array	Replication	Host	LUN ID	Array
cellarfodb1-tz (data)	300	NL-SAS/RAID6	30	1E	Lava	338	VNX_0475	 	 		 
cellarfodb1-tz (data)	300	NL-SAS/RAID6	31	1F	Lava	339	VNX_0475	 	 		 
cellarfodb1-tz (db)	200	SAS/RAID5	32	20	Lava	340	VNX_0475	 	 		 
cellarfodb1-tz (db)	200	SAS/RAID5	33	21	Lava	341	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	34	22	Lava	342	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	35	23	Lava	343	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	36	24	Lava	344	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	37	25	Lava	345	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	38	26	Lava	346	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	39	27	Lava	347	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	40	28	Lava	348	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	41	29	Lava	349	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraredo	40	SAS/RAID5	42	2A	Lava	350	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraredo	40	SAS/RAID5	43	2B	Lava	351	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	44	2C	Lava	352	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	45	2D	Lava	353	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	46	2E	Lava	354	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	47	2F	Lava	355	VNX_0475	 	 		 



#######################################################################
##### check lun visiblity


0[131213/095524]root@lava# /home/betorma/bin/luxadm_carlo -z | egrep ' 30 | 31 | 32 | 33 | 34 | 35 | 36 | 37 | 38 | 39 | 40 | 41 | 42 | 43 | 44 | 45 | 46 | 47' | sort -k 4
/dev/rdsk/c0t6006016017602D0038FD0175A361E311d0s2 ==> 500601603ea42f4a 30 500601623ea42f4a 30 500601683ea42f4a 30 5006016a3ea42f4a 30  []
/dev/rdsk/c0t6006016017602D003AFD0175A361E311d0s2 ==> 500601603ea42f4a 31 500601623ea42f4a 31 500601683ea42f4a 31 5006016a3ea42f4a 31  []
/dev/rdsk/c0t6006016017602D007C48FD94A361E311d0s2 ==> 500601603ea42f4a 32 500601623ea42f4a 32 500601683ea42f4a 32 5006016a3ea42f4a 32  []
/dev/rdsk/c0t6006016017602D007E48FD94A361E311d0s2 ==> 500601603ea42f4a 33 500601623ea42f4a 33 500601683ea42f4a 33 5006016a3ea42f4a 33  []
/dev/rdsk/c0t6006016017602D008048FD94A361E311d0s2 ==> 500601603ea42f4a 34 500601623ea42f4a 34 500601683ea42f4a 34 5006016a3ea42f4a 34  []
/dev/rdsk/c0t6006016017602D008248FD94A361E311d0s2 ==> 500601603ea42f4a 35 500601623ea42f4a 35 500601683ea42f4a 35 5006016a3ea42f4a 35  []
/dev/rdsk/c0t6006016017602D008448FD94A361E311d0s2 ==> 500601603ea42f4a 36 500601623ea42f4a 36 500601683ea42f4a 36 5006016a3ea42f4a 36  []
/dev/rdsk/c0t6006016017602D008648FD94A361E311d0s2 ==> 500601603ea42f4a 37 500601623ea42f4a 37 500601683ea42f4a 37 5006016a3ea42f4a 37  []
/dev/rdsk/c0t6006016017602D008848FD94A361E311d0s2 ==> 500601603ea42f4a 38 500601623ea42f4a 38 500601683ea42f4a 38 5006016a3ea42f4a 38  []
/dev/rdsk/c0t6006016017602D008A48FD94A361E311d0s2 ==> 500601603ea42f4a 39 500601623ea42f4a 39 500601683ea42f4a 39 5006016a3ea42f4a 39  []
/dev/rdsk/c0t6006016017602D008C48FD94A361E311d0s2 ==> 500601603ea42f4a 40 500601623ea42f4a 40 500601683ea42f4a 40 5006016a3ea42f4a 40  []
/dev/rdsk/c0t6006016017602D008E48FD94A361E311d0s2 ==> 500601603ea42f4a 41 500601623ea42f4a 41 500601683ea42f4a 41 5006016a3ea42f4a 41  []
/dev/rdsk/c0t6006016017602D00F41BC7B6A361E311d0s2 ==> 500601603ea42f4a 42 500601623ea42f4a 42 500601683ea42f4a 42 5006016a3ea42f4a 42  []
/dev/rdsk/c0t6006016017602D00F61BC7B6A361E311d0s2 ==> 500601603ea42f4a 43 500601623ea42f4a 43 500601683ea42f4a 43 5006016a3ea42f4a 43  []
/dev/rdsk/c0t6006016017602D0076132DCDA361E311d0s2 ==> 500601603ea42f4a 44 500601623ea42f4a 44 500601683ea42f4a 44 5006016a3ea42f4a 44  []
/dev/rdsk/c0t6006016017602D0078132DCDA361E311d0s2 ==> 500601603ea42f4a 45 500601623ea42f4a 45 500601683ea42f4a 45 5006016a3ea42f4a 45  []
/dev/rdsk/c0t6006016017602D007A132DCDA361E311d0s2 ==> 500601603ea42f4a 46 500601623ea42f4a 46 500601683ea42f4a 46 5006016a3ea42f4a 46  []
/dev/rdsk/c0t6006016017602D007C132DCDA361E311d0s2 ==> 500601603ea42f4a 47 500601623ea42f4a 47 500601683ea42f4a 47 5006016a3ea42f4a 47  []








#######################################################################
##### zpool creation
##### on first node




 	Size	Lun	 	Master Device	 	 	Replica Device
Device	GB	Type	Dec	Hexa	Host	LUN ID	Array	Replication	Host	LUN ID	Array
cellarfodb1-tz (data)	300	NL-SAS/RAID6	30	1E	Lava	338	VNX_0475	 	 		 
cellarfodb1-tz (data)	300	NL-SAS/RAID6	31	1F	Lava	339	VNX_0475	 	 		 

/dev/rdsk/c0t6006016017602D0038FD0175A361E311d0s2 ==> 500601603ea42f4a 30 500601623ea42f4a 30 500601683ea42f4a 30 5006016a3ea42f4a 30  []
/dev/rdsk/c0t6006016017602D003AFD0175A361E311d0s2 ==> 500601603ea42f4a 31 500601623ea42f4a 31 500601683ea42f4a 31 5006016a3ea42f4a 31  []



0[131213/101932]root@lava# zpool create cellarfodb1-tz-data c0t6006016017602D0038FD0175A361E311d0 c0t6006016017602D003AFD0175A361E311d0
'cellarfodb1-tz-data' successfully created, but with no redundancy; failure of one
device will cause loss of the pool
0[131213/101936]root@lava# zpool status $zpool
  pool: cellarfodb1-tz-data
 state: ONLINE
  scan: none requested
config:

        NAME                                     STATE     READ WRITE CKSUM
        cellarfodb1-tz-data                      ONLINE       0     0     0
          c0t6006016017602D0038FD0175A361E311d0  ONLINE       0     0     0
          c0t6006016017602D003AFD0175A361E311d0  ONLINE       0     0     0

errors: No known data errors










 	Size	Lun	 	Master Device	 	 	Replica Device
Device	GB	Type	Dec	Hexa	Host	LUN ID	Array	Replication	Host	LUN ID	Array
cellarfodb1-tz (db)	200	SAS/RAID5	32	20	Lava	340	VNX_0475	 	 		 
cellarfodb1-tz (db)	200	SAS/RAID5	33	21	Lava	341	VNX_0475	 	 		 


/dev/rdsk/c0t6006016017602D007C48FD94A361E311d0s2 ==> 500601603ea42f4a 32 500601623ea42f4a 32 500601683ea42f4a 32 5006016a3ea42f4a 32  []
/dev/rdsk/c0t6006016017602D007E48FD94A361E311d0s2 ==> 500601603ea42f4a 33 500601623ea42f4a 33 500601683ea42f4a 33 5006016a3ea42f4a 33  []



2[131213/102139]root@lava# zpool create cellarfodb1-tz-db c0t6006016017602D007C48FD94A361E311d0 c0t6006016017602D007E48FD94A361E311d0
'cellarfodb1-tz-db' successfully created, but with no redundancy; failure of one
device will cause loss of the pool
0[131213/102144]root@lava# zpool status cellarfodb1-tz-db
  pool: cellarfodb1-tz-db
 state: ONLINE
  scan: none requested
config:

        NAME                                     STATE     READ WRITE CKSUM
        cellarfodb1-tz-db                        ONLINE       0     0     0
          c0t6006016017602D007C48FD94A361E311d0  ONLINE       0     0     0
          c0t6006016017602D007E48FD94A361E311d0  ONLINE       0     0     0

errors: No known data errors












#############################################################################################################################
##### zfs


export data_zpool=cellarfodb1-tz-data
export db_zpool=cellarfodb1-tz-db
export application=cellar


zfs set mountpoint=/zpool/${data_zpool} ${data_zpool}
zfs set mountpoint=/zpool/${db_zpool} ${db_zpool}

zfs create -o mountpoint=none -o zoned=on ${data_zpool}/applications
zfs create -o mountpoint=none -o zoned=on ${data_zpool}/applications/${application}
zfs create -o mountpoint=/applications/${application}/users -o zoned=on ${data_zpool}/applications/${application}/users
zfs create -o mountpoint=/applications/${application}/xchange -o zoned=on ${data_zpool}/applications/${application}/xchange
zfs create -o mountpoint=/u01/oraagent -o zoned=on ${data_zpool}/applications/oraagent
zfs create -o mountpoint=/zones/cellarfodb1-tz cellarfodb1-tz-data/zone



zfs create -o mountpoint=none -o zoned=on ${db_zpool}/applications
zfs create -o mountpoint=none -o zoned=on ${db_zpool}/applications/${application}
zfs create -o mountpoint=/applications/${application}/orabin -o zoned=on ${db_zpool}/applications/${application}/orabin
zfs create -o mountpoint=/applications/${application}/oradata -o zoned=on -o recordsize=8k ${db_zpool}/applications/${application}/oradata
zfs create -o mountpoint=/applications/${application}/oralog -o zoned=on ${db_zpool}/applications/${application}/oralog
zfs create -o mountpoint=/applications/${application}/oraonlinelog -o zoned=on ${db_zpool}/applications/${application}/oraonlinelog
zfs create -o mountpoint=/applications/${application}/oraflash -o zoned=on ${db_zpool}/applications/${application}/oraflash






















#############################################################################################################################
##### zone creation
##### on global zone


pkg publisher | grep 'http://pkg.opoce.cec.eu.int:10001/solaris'



zone=cellarfodb1-tz
zone_ip=158.167.224.172
netmask='/26'
zone_defrouter=158.167.224.190
vlan_id=222
application_ip=158.167.224.173
global_network_interface=aggr1
db_zpool=cellarfodb1-tz-db
data_zpool=cellarfodb1-tz-data
tmp_folder=/home/betorma/export/zones/${zone}
mkdir -p ${tmp_folder}





cat <<EOF >${tmp_folder}/${zone}.cfg
create -b
set brand=solaris
set zonepath=/zones/${zone}
set autoboot=false
set bootargs="-m verbose"
set ip-type=exclusive
add anet
set linkname=net0_${vlan_id}
set lower-link=aggr1
set allowed-address=${zone_ip}${netmask},${application_ip}${netmask}
set defrouter=${zone_defrouter}
set vlan-id=${vlan_id}
set mac-address=random
set configure-allowed-address=true
end
add dataset
set name=${db_zpool}/applications
set alias="${db_zpool}_applications"
end
add dataset
set name=${data_zpool}/applications
set alias="${data_zpool}_applications"
end
add attr
set name=comment
set type=string
set value="Solaris 11 zone for cordis ica"
end
EOF

cat ${tmp_folder}/${zone}.cfg


mkdir -p /zones/${zone}
chmod 700 /zones/${zone}
zonecfg -z ${zone} -f ${tmp_folder}/${zone}.cfg
sysconfig create-profile -o ${tmp_folder}/${zone}_sysconfig.xml

zoneadm -z ${zone} install -c ${tmp_folder}/${zone}_sysconfig.xml





zoneadm -z $zone boot && zlogin -C $zone



#############################################################################################################################
##### zone configuration
##### on the zone


zone_ip=158.167.224.172
application_ip=158.167.224.173


##### password

passwd root

cp -p /etc/default/passwd /etc/default/passwd.orig
vi /etc/default/passwd

diff /etc/default/passwd /etc/default/passwd.orig
14c14
< NAMECHECK=NO
---
> #NAMECHECK=NO
41c41
< MINDIGIT=0
---
> #MINDIGIT=0


##### name resolution

{
cat <<EOF >/etc/defaultdomain
opoce.cec.eu.int
EOF
}


{
cat <<EOF >/etc/resolv.conf
domain opoce.cec.eu.int
nameserver 158.167.99.8
nameserver 158.167.99.7
search opoce.cec.eu.int
EOF
}


nscfg import -fv svc:/network/dns/client:default
svcadm refresh svc:/network/dns/client:default


{
cat <<EOF >/etc/nsswitch.conf
passwd:         files ldap
group:          files ldap
hosts:          files dns
ipnodes:        files dns
networks:       files ldap
protocols:      files ldap
rpc:            files ldap
ethers:         files ldap
netmasks:       files ldap
bootparams:     files ldap
publickey:      files ldap
netgroup:       files ldap
automount:      files ldap
aliases:        files ldap
services:       files ldap
printers:       user files ldap
project:        files ldap
auth_attr:      files ldap
prof_attr:      files ldap
tnrhtp:         files ldap
tnrhdb:         files ldap
sudoers:        files ldap
EOF
}


nscfg import -fv name-service/switch
svcadm refresh name-service/switch


svcadm restart dns/client




{
cat <<EOF >/etc/inet/hosts
::1 localhost
127.0.0.1 localhost
`getent hosts $zone_ip | sed -e s/.opoce.cec.eu.int//`
`getent hosts $application_ip | sed -e s/.opoce.cec.eu.int//`
EOF
echo
cat /etc/inet/hosts
}



##### ldap

{
cat <<EOF >/var/ldap/ldap_client_cred
NS_LDAP_BINDDN= cn=proxyagent,ou=profile,dc=opoce,dc=cec,dc=eu,dc=int
NS_LDAP_BINDPASSWD= {NS1}135a8787c1cf6353f4
EOF
}


{
cat <<EOF >/var/ldap/ldap_client_file
NS_LDAP_FILE_VERSION= 2.0
NS_LDAP_SERVERS= 158.167.99.8, 158.167.99.7
NS_LDAP_SEARCH_BASEDN= dc=opoce,dc=cec,dc=eu,dc=int
NS_LDAP_AUTH= simple
NS_LDAP_SEARCH_REF= FALSE
NS_LDAP_SEARCH_SCOPE= one
NS_LDAP_SEARCH_TIME= 30
NS_LDAP_CACHETTL= 43200
NS_LDAP_PROFILE= drp
NS_LDAP_CREDENTIAL_LEVEL= proxy
NS_LDAP_BIND_TIME= 10
EOF
}

chmod 400 /var/ldap/ldap_client_cred /var/ldap/ldap_client_file

svcadm enable svc:/network/nis/domain:default

nscfg import -fv ldap/client
svcadm refresh ldap/client
svcadm enable ldap/client

ldaplist



##### packages

{
pkg install pkg:/shell/zsh@4.3.17-0.175.1.0.0.24.0
pkg install pkg:/archiver/gnu-tar@1.26-0.175.1.0.0.24.0
pkg install pkg:/terminal/xterm@271-0.175.1.0.0.24.1317
pkg install pkg:/system/management/snmp/net-snmp@5.4.1-0.175.1.0.0.24.0
pkg install pkg:/system/management/snmp/net-snmp/addons@5.4.1-0.175.1.0.0.24.0
pkg install pkg:/service/network/ftp@1.3.3.0.7-0.175.1.0.0.24.0
pkg install pkg:/network/ftp@0.5.11-0.175.1.0.0.24.2
}

{
pkg info pkg:/shell/zsh@4.3.17-0.175.1.0.0.24.0
pkg info pkg:/archiver/gnu-tar@1.26-0.175.1.0.0.24.0
pkg info pkg:/terminal/xterm@271-0.175.1.0.0.24.1317
pkg info pkg:/system/management/snmp/net-snmp@5.4.1-0.175.1.0.0.24.0
pkg info pkg:/system/management/snmp/net-snmp/addons@5.4.1-0.175.1.0.0.24.0
pkg info pkg:/service/network/ftp@1.3.3.0.7-0.175.1.0.0.24.0
pkg info pkg:/network/ftp@0.5.11-0.175.1.0.0.24.2
} | egrep 'Name:|State:'


##### snmp

# from the global zone
cp  /etc/net-snmp/snmp/snmpd.conf /zones/${zone}/root/etc/net-snmp/snmp/

# from the non-global zone
svcadm enable net-snmp

# from monitoring server
zone=cellarfodb1-tz
snmpwalk -c specOPOCE -v2c $zone


##### mail

cp -p /etc/mail/sendmail.cf /etc/mail/sendmail.cf.orig
vi /etc/mail/sendmail.cf

diff /etc/mail/sendmail.cf /etc/mail/sendmail.cf.orig
89c89
< DSsmtp:smtp
---
> DS

svcadm restart sendmail
echo `uname -n` | mailx betorma
echo `uname -n` | mailx mathieu.betori@ext.publications.europa.eu



##### ftp


echo 'ftp account required pam_unix_account.so.1' >/etc/pam.d/ftp

# if necessary
svcadm enable svc:/network/ftp:default

# customize /etc/proftpd.conf


##### role

echo 'Primary Administrator:suser:cmd:::*:uid=0;gid=0' >>/etc/security/exec_attr.d/core-os

{
echo ''  >> /etc/security/prof_attr.d/core-os
echo 'Primary Administrator:::\' >> /etc/security/prof_attr.d/core-os
echo 'Can perform all administrative tasks:\' >> /etc/security/prof_attr.d/core-os
echo 'auths=solaris.*,solaris.grant;\' >> /etc/security/prof_attr.d/core-os
echo 'help=RtPriAdmin.html' >> /etc/security/prof_attr.d/core-os
}



##### ntp


##### core
coreadm -i /var/cores/%f_%p_%u_%g.core



##### syslog

perl -e 'print "
#
# OPOCE syslog configuration for non-loghost machines
#
*.err;kern.warning;auth.notice;daemon.notice;mail.crit\t\t/dev/sysmsg
*.notice;kern.debug;lpr.info;mail.crit;news.err\t\t/var/adm/messages
auth.info;\t\t/var/log/authlog
mail.info;lpr.info\t\t/var/log/syslog
cron.info\t\t/var/log/cron
*.crit\t\troot
*.emerg\t\t*
user.notice;daemon.notice;lpr.notice;news.notice;uucp.notice;audit.notice;kern.debug;mail.crit;auth.notice\t\t@\syslog-srv
"' >/etc/syslog.conf


touch /var/log/cron
svcadm restart svc:/system/system-log:default


















##### backup client

# from global zone
cp /net/opsrv082/xchange/NW_8/802/nw80sp2_solaris_64.tar.gz /zones/$zone/root/var/tmp 

# from the non-global zone
cd /var/tmp/
gunzip nw80sp2_solaris_64.tar.gz 
tar xf nw80sp2_solaris_64.tar
cd solaris_64/ 
pkgadd -d . LGTOman LGTOclnt

Do you want to continue with the installation of <LGTOman> [y,n,?] y
Enter a NetWorker data directory (default=/var/nsr) [?] /nsr
Enter a NetWorker server hostname [no more]: opvmwsbkp01
Enter a NetWorker server hostname [no more]:        
Restart NetWorker daemons at end of install? (default=no) [y,n,?] y
Do you want to continue with the installation of <LGTOclnt> [y,n,?] y





##### last check

reboot
svcs -xv
dmesg








#############################################################################################################################
##### application users



##### variables
export appli_project=cellar
export appli_project_id=1860
export appli_user=cellar
export appli_uid=83400
export comment_appli_user="${appli_user} user for ${appli_project} project"
export w_appli_user="w_${appli_user}"
export w_appli_uid=83401
export comment_w_appli_user="${w_appli_user} wood user for ${appli_project} project"
export appli_group=cellar
export appli_gid=84700
export oracle_used=yes
export documentum_used=no
export test_used=no

##### sauvegarde des fichiers a modifier
{
for FILE in /etc/auto_home /etc/group /etc/passwd /etc/shadow /etc/user_attr /etc/security/exec_attr /etc/security/prof_attr /etc/project
do
	cp $FILE $FILE.`date +%Y%m%d%H%M`
done
}

##### ajout des roles dba et oracle
{
mkdir -p /u01/home/oracle
mkdir -p /u01/home/rootdba
mkdir -p /u02
echo "rootdba::::type=role;profiles=Primary Administrator" >>/etc/user_attr
echo "oracle::::type=role;profiles=OraAgent Management,All" >>/etc/user_attr
echo "oracle     \$HOST:/u01/home/&" >>/etc/auto_home
echo "rootdba      \$HOST:/u01/home/&" >>/etc/auto_home
echo 'dba::55:oracle' >>/etc/group
echo 'oracle:x:55:55:Oracle Role:/home/oracle:/bin/pfksh' >>/etc/passwd
echo 'rootdba:x:20000:1:DBA Role:/home/rootdba:/bin/pfksh' >>/etc/passwd
echo 'oracle:N1adVIyiQ/ufM:12577::::::' >>/etc/shadow
echo 'rootdba:c1B14rQDdgzPY:12500::::::' >>/etc/shadow
pwconv
chown 55:55 /u01/home/oracle
}

{
if [[ ${oracle_used} == yes ]]
then
	for rep in orabin oradata oralog oraflash
	do
		if [ -d /applications/${appli_project}/$rep ]
		then
			chown 55:55 /applications/${appli_project}/$rep
               ls -ld /applications/${appli_project}/$rep
		fi
	done
	mkdir /var/opt/oracle
	chown 55:55 /var/opt/oracle
fi
}

##### creation de $appli_group
{
grep ^${appli_group} /etc/group
groupadd -g ${appli_gid} ${appli_group}
egrep "^${appli_group}|${appli_gid}" /etc/group
}

##### creation de $appli_user
{
grep ^${appli_user} /etc/passwd
grep ^${appli_user} /etc/user_attr
mkdir -p /applications/${appli_project}/users/${appli_user}
roleadd -d /home/${appli_user} -c "${comment_appli_user}" -u ${appli_uid} -g ${appli_group} -s /bin/pfksh  ${appli_user} 
egrep "^${appli_user}|${appli_uid}" /etc/passwd
}

##### creation de wood_group 
{
grep ^wood /etc/group
groupadd -g 65535 wood
egrep "^wood|65535" /etc/group
}

##### creation de w_appli_user
{
grep ^${w_appli_user} /etc/passwd
grep ^${w_appli_user} /etc/user_attr
mkdir -p /applications/${appli_project}/users/${w_appli_user}
roleadd -d /home/${w_appli_user} -c "${comment_w_appli_user}" -u ${w_appli_uid} -g ${appli_group} -G wood -s /bin/pfksh ${w_appli_user}
egrep "^${w_appli_user}|${w_appli_uid}" /etc/passwd
}

##### auto_home de $appli_user
{
echo "${appli_user}     \$HOST:/applications/${appli_project}/users/&" >> /etc/auto_home
echo "${w_appli_user}     \$HOST:/applications/${appli_project}/users/&" >> /etc/auto_home
cat /etc/auto_home
}

##### modification des droit sur le home de $appli_user
{
chown ${appli_user}:${appli_group} /applications/${appli_project}/users/${appli_user}
ls -ld /applications/${appli_project}/users/${appli_user}
chown ${w_appli_user}:${appli_group} /applications/${appli_project}/users/${w_appli_user}
ls -ld /applications/${appli_project}/users/${w_appli_user}
}

##### changement du mot de passe de $appli_user
passwd ${appli_user}
passwd ${w_appli_user}

passwd: Password cannot be circular shift of logonid.


##### creation des utilisateurs pour documentum
{
if [[ ${documentum_used} == yes ]]
then
	mkdir -p /applications/${appli_project}/users/dmadmin
	mkdir -p /applications/${appli_project}/users/pdocu
	mkdir -p /applications/${appli_project}/users/docuser
	roleadd -d /home/dmadmin -u 81800 -g ${appli_group} -s /bin/pfksh dmadmin
	roleadd -d /home/pdocu -u 81801 -g ${appli_group} -s /bin/pfksh pdocu
	roleadd -d /home/docuser -u 81802 -g ${appli_group} -s /bin/pfksh docuser
	echo "dmadmin     \$HOST:/applications/${appli_project}/users/&" >>/etc/auto_home
	echo "pdocu     \$HOST:/applications/${appli_project}/users/&" >>/etc/auto_home
	echo "docuser     \$HOST:/applications/${appli_project}/users/&" >>/etc/auto_home
	chown dmadmin:${appli_group} /applications/${appli_project}/users/dmadmin
	chown pdocu:${appli_group} /applications/${appli_project}/users/pdocu
	chown docuser:${appli_group} /applications/${appli_project}/users/docuser
fi
}






##### ajout des acces pour l' equipes integration de test au role $appli_user
{
if [[ ${test_used} == yes ]]
then
	for user in maffima klaerpa lafarpa niedema holotma pierrph dotzech naratol
	do
		echo "${user}::::type=normal;roles=${appli_user},${w_appli_user}" >>/etc/user_attr
	done
fi
cat /etc/user_attr
}

########### exec_attr
{
echo "${appli_project} Management:suser:cmd:::/applications/${appli_project}/users/system/init.d/*:uid=0" >>/etc/security/exec_attr
cat /etc/security/exec_attr
}

#### /etc/security/prof_attr
{
echo "${appli_project} Management:::${appli_project} start/stop:auths=solaris.smf.manage.applications/${appli_project}" >>/etc/security/prof_attr
echo "OraAgent Management:::OraAgent profile:auths=solaris.smf.manage.monitoring/oraagent" >>/etc/security/prof_attr
cat /etc/security/prof_attr
}

####  /etc/project
{
cat <<EOF >/etc/project
system:0::::
user.root:1::::
noproject:2::::
default:3::::
group.staff:10::::
user.${appli_project}:${appli_project_id}:${appli_project}:${appli_user}:${appli_group},staff:
${appli_project}.app:${appli_project_id%0}1:${appli_project}:${appli_user}:${appli_group},staff:
${appli_project}.dba:${appli_project_id%0}2:${appli_project}:${appli_user},oracle:dba:process.max-file-descriptor=(basic,1024,deny);project.max-shm-memory=(priv,4294967296,deny)
${appli_project}.wood:${appli_project_id%0}3:${appli_project}:${appli_user},${w_appli_user}:${appli_group}:
${appli_project}.woodweb:${appli_project_id%0}4:${appli_project}:${appli_user},${w_appli_user}:${appli_group}:
EOF
cat /etc/project
}

##### droit sur le repertoire xchange

{
if [ -d /applications/${appli_project}/xchange ]
then
	chown ${appli_user}:${appli_group} /applications/${appli_project}/xchange
	ls -ld /applications/${appli_project}/xchange
fi
}


##### connexion aux roles
{
for users in ${appli_user} ${w_appli_user} rootdba oracle dmadmin pdocu docuser
do
	echo "##### $users"
	su - $users -c 'id;pwd'
	echo	
done
}






#############################################################################################################################
##### capping







#############################################################################################################################
##### cluster


##### beadm





#######################################################################
##### cluster srdf configuration
##### on first node

d13                 groucho:/dev/rdsk/c0t60000970000292603453533031313041d0
d21                 chico:/dev/rdsk/c0t60000970000292602560533031313041d0




36[131128/113843]root@groucho# cldg offline dsk/d13 dsk/d21 
3[131128/114022]root@groucho# cldg disable dsk/d13 dsk/d21 
0[131128/114027]root@groucho# cldg delete dsk/d13 dsk/d21 


#######################################################################
##### cluster srdf configuration
##### on second node

0[131128/113806]root@chico# cldevice combine -t srdf -g $sym_dg -d d13 d21 

0[131128/141052]root@chico# cldevice show d13

=== DID Device Instances ===                   

DID Device Name:                                /dev/did/rdsk/d13
  Full Device Path:                                groucho:/dev/rdsk/c0t60000970000292603453533031313041d0
  Full Device Path:                                chico:/dev/rdsk/c0t60000970000292602560533031313041d0
  Replication:                                     srdf
  default_fencing:                                 scsi3


0[131128/141105]root@chico# didadm -F scsi3 d13

0[131128/141427]root@chico# cldg create -n groucho,chico -t rawdisk -d d13 $zpool
0[131128/141431]root@chico# cldg show $zpool                                     

=== Device Groups ===                          

Device Group Name:                              cellarfodb1-tz-data
  Type:                                            Disk
  failback:                                        false
  Node List:                                       groucho, chico
  preferenced:                                     false
  localonly:                                       false
  autogen:                                         false
  numsecondaries:                                  1
  device names:                                    /dev/did/rdsk/d13s2
  Replication Type:                                srdf


#######################################################################
##### cluster resources configuration
##### on first node

export rg=${zone}-rg
echo $rg

clrg create $rg
clrg manage $rg
clrg online $rg
clrg switch -n `uname -n` $rg

clresource create -g $rg -t SUNW.HAStoragePlus -p GlobalDevicePaths=$zpool $zpool-srdf
clresource create -g $rg -t SUNW.HAStoragePlus -p Resource_dependencies=$zpool-srdf -p zpools=$zpool $zpool-zfs
clrs status -g $rg   








#######################################################################
##### cluster srdf configuration
##### on first node


d12                 groucho:/dev/rdsk/c0t60000970000292603453533030374446d0
d20                 chico:/dev/rdsk/c0t60000970000292602560533030374446d0

0[131128/143949]root@groucho# cldg offline dsk/d12 dsk/d20
0[131128/144242]root@groucho# cldg disable dsk/d12 dsk/d20
0[131128/144244]root@groucho# cldg delete dsk/d12 dsk/d20
0[131128/144246]root@groucho# 




#######################################################################
##### cluster srdf configuration
##### on second node

0[131128/144223]root@chico# cldevice combine -t srdf -g $sym_dg -d d12 d20
0[131128/144303]root@chico# cldevice show d12

=== DID Device Instances ===                   

DID Device Name:                                /dev/did/rdsk/d12
  Full Device Path:                                groucho:/dev/rdsk/c0t60000970000292603453533030374446d0
  Full Device Path:                                chico:/dev/rdsk/c0t60000970000292602560533030374446d0
  Replication:                                     srdf
  default_fencing:                                 global



0[131128/144310]root@chico# didadm -F scsi3 d12
0[131128/144329]root@chico# 

cldg create -n groucho,chico -t rawdisk -d d12 $zpool
cldg show $zpool                                     



0[131128/144345]root@chico# cldg show $zpool                                     

=== Device Groups ===                          

Device Group Name:                              cellarfodb1-tz-db
  Type:                                            Disk
  failback:                                        false
  Node List:                                       groucho, chico
  preferenced:                                     false
  localonly:                                       false
  autogen:                                         false
  numsecondaries:                                  1
  device names:                                    /dev/did/rdsk/d12s2
  Replication Type:                                srdf


#######################################################################
##### cluster resources configuration
##### on first node


0[131128/144720]root@groucho# clrs disable cellarfodb1-tz-data-zfs
0[131128/144731]root@groucho# clrs delete cellarfodb1-tz-data-zfs

0[131128/144738]root@groucho# echo clrs create -g $rg -t SUNW.HAStoragePlus -p GlobalDevicePaths=$zpool $zone-srdf
clrs create -g cellarfodb1-tz-rg -t SUNW.HAStoragePlus -p GlobalDevicePaths=cellarfodb1-tz-db cellarfodb1-tz-srdf
0[131128/144748]root@groucho# clrs create -g cellarfodb1-tz-rg -t SUNW.HAStoragePlus -p GlobalDevicePaths=cellarfodb1-tz-db,cellarfodb1-tz-data cellarfodb1-tz-srdf


0[131128/144810]root@groucho# clrs disable  cellarfodb1-tz-data-srdf
0[131128/144910]root@groucho# clrs delete cellarfodb1-tz-data-srdf 

0[131128/144937]root@groucho# echo clresource create -g $rg -t SUNW.HAStoragePlus -p Resource_dependencies=$zpool-srdf -p zpools=$zpool $zone-zfs
clresource create -g cellarfodb1-tz-rg -t SUNW.HAStoragePlus -p Resource_dependencies=cellarfodb1-tz-db-srdf -p zpools=cellarfodb1-tz-db cellarfodb1-tz-zfs


36[131128/145059]root@groucho# clresource create -g cellarfodb1-tz-rg -t SUNW.HAStoragePlus -p Resource_dependencies=cellarfodb1-tz-srdf -p zpools=cellarfodb1-tz-db,cellarfodb1-tz-data cellarfodb1-tz-zfs 



#############################################################################################################################
##### cluster zone 
##### on primary node

export zname=cellarfodb1-tz
export rgname=${zone}-rg
export rsnamestorage=${zone}-zfs
export ztype=solaris
export server_source=groucho
export server_target=chico



zlogin $zone init 0 && zlogin -C $zone 
~~.


clresourcetype list
# if necessary:
#clresourcetype register SUNW.HAStoragePlus
#clresourcetype register SUNW.gds


cat <<EOT >/opt/SUNWsczone/sczbt/util/sczbt_${zname}
RS=${zname}-rs
RG=${rgname}
PARAMETERDIR=/etc/zones
SC_NETWORK=false
SC_LH=
FAILOVER=true
HAS_RS=${rsnamestorage}
Zonename="${zname}"
Zonebrand="${ztype}"
Zonebootopt=""
Milestone="multi-user-server"
LXrunlevel="3"
SLrunlevel="3"
Mounts=""
EOT

cat /opt/SUNWsczone/sczbt/util/sczbt_${zname}


zonecfg -z $zone <<EOF
add attr
set name=osc-ha-zone
set type=boolean
set value=true
end
EOF





cp -p /opt/SUNWsczone/sczbt/util/sczbt_${zname} /opt/SUNWsczone/sczbt/util/sczbt_config
cd /opt/SUNWsczone/sczbt/util
./sczbt_register -f /opt/SUNWsczone/sczbt/util/sczbt_config
clrg status $rgname
clrs status -g $rgname
clrs enable ${zname}-rs
clrs status ${zname}-rs
clrs unmonitor ${zname}-rs
clrs status ${zname}-rs


export tmp_dir=/net/opsrv082/xchange/lc/cluster
mkdir -p $tmp_dir
cd /opt/SUNWsczone/sczbt/util && tar cvfp $tmp_dir/sczbt_${zname}.tar sczbt_${zname}
cd /etc && tar cvfp $tmp_dir/zones.tar zones


# on secondary node
export tmp_dir=/net/opsrv082/xchange/lc/cluster
export zname=cellarfodb1-tz
cd /opt/SUNWsczone/sczbt/util && tar xvfp $tmp_dir/sczbt_${zname}.tar 
cd /etc && tar xvfp $tmp_dir/zones.tar



timex clrg switch -n $server_target $rgname
clrs status
timex clrg switch -n $server_source $rgname
clrs status

#########################################################################################################################################
##### asm




 	Size	Lun	 	Master Device	 	 	Replica Device
Device	GB	Type	Dec	Hexa	Host	LUN ID	Array	Replication	Host	LUN ID	Array
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	34	22	Lava	342	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	35	23	Lava	343	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	36	24	Lava	344	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	37	25	Lava	345	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	38	26	Lava	346	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	39	27	Lava	347	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	40	28	Lava	348	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oradata	200	SAS/RAID5	41	29	Lava	349	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraredo	40	SAS/RAID5	42	2A	Lava	350	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraredo	40	SAS/RAID5	43	2B	Lava	351	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	44	2C	Lava	352	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	45	2D	Lava	353	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	46	2E	Lava	354	VNX_0475	 	 		 
cellarfodb1-tz-ASM-oraFRA	50	SAS/RAID5	47	2F	Lava	355	VNX_0475	 	 		 



##### preparatifs

export tmp_folder=/home/betorma/tmp
export zone=cellarfodb1-tz

export tag=oraredo
/home/betorma/bin/luxadm_carlo| egrep ' 42 | 43 ' >${tmp_folder}/asm_${zone}_${tag}.txt
cat ${tmp_folder}/asm_${zone}_${tag}.txt


export tag=orafra
/home/betorma/bin/luxadm_carlo| egrep ' 44 | 45 | 46 | 47 ' >${tmp_folder}/asm_${zone}_${tag}.txt
cat ${tmp_folder}/asm_${zone}_${tag}.txt                                                      


export tag=oradata
/home/betorma/bin/luxadm_carlo| egrep ' 48 | 49 | 50 | 51 | 52 | 53 | 54 | 55 ' >${tmp_folder}/asm_${zone}_${tag}.txt
cat ${tmp_folder}/asm_${zone}_${tag}.txt                                                                                 



0[131216/143826]root@lava# export tag=oradata
0[131216/143928]root@lava# /home/betorma/bin/luxadm_carlo| egrep ' 34 | 35 | 36 | 37 | 38 | 39 | 40 | 41 ' >${tmp_folder}/asm_${zone}_${tag}.txt
0[131216/143931]root@lava# cat ${tmp_folder}/asm_${zone}_${tag}.txt
path: /dev/rdsk/c0t6006016017602D008A48FD94A361E311d0s2 ==> stor: 5006016a3ea42f4a lun 39 stor: 500601683ea42f4a lun 39 stor: 500601623ea42f4a lun 39 stor: 500601603ea42f4a lun 39 
path: /dev/rdsk/c0t6006016017602D008C48FD94A361E311d0s2 ==> stor: 5006016a3ea42f4a lun 40 stor: 500601683ea42f4a lun 40 stor: 500601623ea42f4a lun 40 stor: 500601603ea42f4a lun 40 
path: /dev/rdsk/c0t6006016017602D008E48FD94A361E311d0s2 ==> stor: 5006016a3ea42f4a lun 41 stor: 500601683ea42f4a lun 41 stor: 500601623ea42f4a lun 41 stor: 500601603ea42f4a lun 41 
path: /dev/rdsk/c0t6006016017602D008048FD94A361E311d0s2 ==> stor: 5006016a3ea42f4a lun 34 stor: 500601683ea42f4a lun 34 stor: 500601623ea42f4a lun 34 stor: 500601603ea42f4a lun 34 
path: /dev/rdsk/c0t6006016017602D008248FD94A361E311d0s2 ==> stor: 5006016a3ea42f4a lun 35 stor: 500601683ea42f4a lun 35 stor: 500601623ea42f4a lun 35 stor: 500601603ea42f4a lun 35 
path: /dev/rdsk/c0t6006016017602D008448FD94A361E311d0s2 ==> stor: 500601683ea42f4a lun 36 stor: 5006016a3ea42f4a lun 36 stor: 500601623ea42f4a lun 36 stor: 500601603ea42f4a lun 36 
path: /dev/rdsk/c0t6006016017602D008648FD94A361E311d0s2 ==> stor: 500601683ea42f4a lun 37 stor: 5006016a3ea42f4a lun 37 stor: 500601623ea42f4a lun 37 stor: 500601603ea42f4a lun 37 
path: /dev/rdsk/c0t6006016017602D008848FD94A361E311d0s2 ==> stor: 500601683ea42f4a lun 38 stor: 5006016a3ea42f4a lun 38 stor: 500601623ea42f4a lun 38 stor: 500601603ea42f4a lun 38 


0[131216/143933]root@lava# export tag=oraredo
0[131216/143955]root@lava# /home/betorma/bin/luxadm_carlo| egrep ' 42 | 43 ' >${tmp_folder}/asm_${zone}_${tag}.txt
0[131216/143956]root@lava# cat ${tmp_folder}/asm_${zone}_${tag}.txt
path: /dev/rdsk/c0t6006016017602D00F41BC7B6A361E311d0s2 ==> stor: 5006016a3ea42f4a lun 42 stor: 500601683ea42f4a lun 42 stor: 500601623ea42f4a lun 42 stor: 500601603ea42f4a lun 42 
path: /dev/rdsk/c0t6006016017602D00F61BC7B6A361E311d0s2 ==> stor: 5006016a3ea42f4a lun 43 stor: 500601683ea42f4a lun 43 stor: 500601623ea42f4a lun 43 stor: 500601603ea42f4a lun 43 


0[131216/143957]root@lava# export tag=orafra
0[131216/144009]root@lava# /home/betorma/bin/luxadm_carlo| egrep ' 44 | 45 | 46 | 47 ' >${tmp_folder}/asm_${zone}_${tag}.txt
0[131216/144010]root@lava# cat ${tmp_folder}/asm_${zone}_${tag}.txt
path: /dev/rdsk/c0t6006016017602D007A132DCDA361E311d0s2 ==> stor: 5006016a3ea42f4a lun 46 stor: 500601683ea42f4a lun 46 stor: 500601623ea42f4a lun 46 stor: 500601603ea42f4a lun 46 
path: /dev/rdsk/c0t6006016017602D007C132DCDA361E311d0s2 ==> stor: 5006016a3ea42f4a lun 47 stor: 500601683ea42f4a lun 47 stor: 500601623ea42f4a lun 47 stor: 500601603ea42f4a lun 47 
path: /dev/rdsk/c0t6006016017602D0076132DCDA361E311d0s2 ==> stor: 5006016a3ea42f4a lun 44 stor: 500601683ea42f4a lun 44 stor: 500601623ea42f4a lun 44 stor: 500601603ea42f4a lun 44 
path: /dev/rdsk/c0t6006016017602D0078132DCDA361E311d0s2 ==> stor: 5006016a3ea42f4a lun 45 stor: 500601683ea42f4a lun 45 stor: 500601623ea42f4a lun 45 stor: 500601603ea42f4a lun 45 





##### recupere la liste des disks

cat ${tmp_folder}/asm_${zone}_ora* | awk '{print $2}' | awk -F'/' '{print $4}' | sed -e 's/s2$//' >${tmp_folder}/asm_disk_list.txt  
cat ${tmp_folder}/asm_disk_list.txt

0[131216/144018]root@lava# cat ${tmp_folder}/asm_disk_list.txt
c0t6006016017602D008A48FD94A361E311d0
c0t6006016017602D008C48FD94A361E311d0
c0t6006016017602D008E48FD94A361E311d0
c0t6006016017602D008048FD94A361E311d0
c0t6006016017602D008248FD94A361E311d0
c0t6006016017602D008448FD94A361E311d0
c0t6006016017602D008648FD94A361E311d0
c0t6006016017602D008848FD94A361E311d0
c0t6006016017602D007A132DCDA361E311d0
c0t6006016017602D007C132DCDA361E311d0
c0t6006016017602D0076132DCDA361E311d0
c0t6006016017602D0078132DCDA361E311d0
c0t6006016017602D00F41BC7B6A361E311d0
c0t6006016017602D00F61BC7B6A361E311d0


#####  ajout des did au fichier de configuration de la zone


export current_date=`date +%Y%m%d%H%M`
echo ${current_date}
201312161440

cp -p /etc/zones/${zone}.xml /etc/zones/${zone}.xml.${current_date} 




{
echo "zonecfg -z ${zone} <<EOT" 
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	echo "add device"
	echo "set match=/dev/did/*dsk/${did}s*"
	echo "end"
done
echo verify
echo commit
echo exit
echo EOT
}


{
zonecfg -z ${zone} <<EOT
`cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	echo "add device"
	echo "set match=/dev/did/*dsk/${did}s*"
	echo "end"
done`
verify
commit
exit
EOT
}



diff /etc/zones/${zone}.xml /etc/zones/${zone}.xml.${current_date} 



##### ajout des disk dynamiquement a la zone via mknode

mkdir -p /zones/${zone}/dev/did/dsk
mkdir -p /zones/${zone}/dev/did/rdsk


{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -lL /dev/did/(dsk|rdsk)/${did}s* | perl -ne 'if(m{(.)rw-------\s+1\s+root\s+sys\s+(\d+),\s+(\d+)\s+.{12}\s+/dev/did/(dsk|rdsk)/(d\d+s\d+)}) {print "mknod /zones/$ENV{zone}/dev/did/$4/$5 $1 $2 $3\n"}'
done
}



{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -lL /dev/did/(dsk|rdsk)/${did}s* | perl -ne 'if(m{(.)rw-------\s+1\s+root\s+sys\s+(\d+),\s+(\d+)\s+.{12}\s+/dev/did/(dsk|rdsk)/(d\d+s\d+)}) {`mknod /zones/$ENV{zone}/dev/did/$4/$5 $1 $2 $3`}'
done
}


{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -l /zones/${zone}/dev/did/*dsk/${did}*
done
}








##### ajout des ACL sur les fichiers devices


{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -lL /dev/did/(dsk|rdsk)/${did}s* | perl -ne 'if(m{(.)rw-------\s+1\s+root\s+sys\s+(\d+),\s+(\d+)\s+.{12}\s+/dev/did/(dsk|rdsk)/(d\d+s\d+)}) {print "chmod u=rw,g=,o= /zones/$ENV{zone}/dev/did/$4/$5\n"}'
done
}


{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -lL /dev/did/(dsk|rdsk)/${did}s* | perl -ne 'if(m{(.)rw-------\s+1\s+root\s+sys\s+(\d+),\s+(\d+)\s+.{12}\s+/dev/did/(dsk|rdsk)/(d\d+s\d+)}) {`chmod u=rw,g=,o= /zones/$ENV{zone}/dev/did/$4/$5`}'
done
}


{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -lL /dev/did/(dsk|rdsk)/${did}s* | perl -ne 'if(m{(.)rw-------\s+1\s+root\s+sys\s+(\d+),\s+(\d+)\s+.{12}\s+/dev/did/(dsk|rdsk)/(d\d+s\d+)}) {print "chown oracle:dba /zones/$ENV{zone}/dev/did/$4/$5\n"}'
done
}


{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -lL /dev/did/(dsk|rdsk)/${did}s* | perl -ne 'if(m{(.)rw-------\s+1\s+root\s+sys\s+(\d+),\s+(\d+)\s+.{12}\s+/dev/did/(dsk|rdsk)/(d\d+s\d+)}) {`chown oracle:dba /zones/$ENV{zone}/dev/did/$4/$5`}'
done
}


{
cldev list -v | egrep -f ${tmp_folder}/asm_disk_list.txt | awk '{print $1}' | uniq | while read did
do
	ls -l /zones/${zone}/dev/did/*dsk/${did}*
done
}



##### partition de s0 avec les 3 premier blocks pour le header asm


{
cat ${tmp_folder}/asm_disk_list.txt | while read disk
do
	echo "############################################# ${disk}"
	done
}



{
cat ${tmp_folder}/asm_disk_list.txt | while read disk
do
	echo "############################################# ${disk}"
	echo dd if=/dev/zero of=/dev/rdsk/${disk} count=1
done
}


{
cat ${tmp_folder}/asm_disk_list.txt | while read disk
do
	echo "############################################# ${disk}"
	format -d ${disk} <<EOT
partition
1
unassigned
wm
0
0

3
unassigned
wm
0
0

4
unassigned
wm
0
0

5
unassigned
wm
0
0

6
unassigned
wm
0
0

7
unassigned
wm
0
0

0
usr
wm
3
$

label
y
quit
quit
EOT
done
}






#### communiquer la liste des did aux dba

rm ${tmp_folder}/asm_disk_list_for_dba.txt

for tag in oradata orafra oraredo
do
	{
	cat ${tmp_folder}/asm_${zone}_${tag}.txt | while read line 
	do
		echo ${line} | awk '{print $2}' | sed -e 's/s2$//' | while read disk
		do
			did=`cldev list -v | grep ${disk} | awk '{print $1}' | uniq`
			echo "disk:${line} did: ${did} zone:$zone [${tag}]"
		done
	done
	} | tee -a ${tmp_folder}/asm_disk_list_for_dba.txt
done




cat ${tmp_folder}/asm_disk_list_for_dba.txt | mailx betorma








0[130523/085035]root@pegase# cat /home/betorma/docs/storage.txt 
NAME            TYPE    WWN                     SITE
stamper         6130    200400A0B818AC1E        Mercier
stamper         6130    200400A0B818AC1F        Mercier
absynthe        6130    200400A0B819E592        Mercier
absynthe        6130    200400A0B819E593        Mercier
peket           6140    200400A0B829991B        Mercier
peket           6140    200400A0B829991C        Mercier
stamper         6130    200500A0B818AC1E        Mercier
stamper         6130    200500A0B818AC1F        Mercier
absynthe        6130    200500A0B819E592        Mercier
absynthe        6130    200500A0B819E593        Mercier
peket           6140    200500A0B829991B        Mercier
peket           6140    200500A0B829991C        Mercier
brizard         6140    200600A0B826DD65        Mercier
brizard         6140    200600A0B826DD66        Mercier
brizard         6140    200700A0B826DD65        Mercier
brizard         6140    200700A0B826DD66        Mercier
gnole           6140    201600A0B8266772        Mercier
gnole           6140    201700A0B8266772        Mercier
torboyaux       6140    201800A0B848F43A        EUFO
torboyaux       6140    201900A0B848F43A        EUFO
gnole           6140    202600A0B8266772        Mercier
gnole           6140    202700A0B8266772        Mercier
torboyaux       6140    202800A0B848F43A        EUFO
torboyaux       6140    202900A0B848F43A        EUFO
gnole           6140    203600A0B8266772        Mercier
gnole           6140    203700A0B8266772        Mercier
torboyaux       6140    203800A0B848F43A        EUFO
torboyaux       6140    203900A0B848F43A        EUFO
gnole           6140    204600A0B8266772        Mercier
gnole           6140    204700A0B8266772        Mercier
torboyaux       6140    204800A0B848F43A        EUFO
torboyaux       6140    204900A0B848F43A        EUFO
VMAX_2560       vmax    5000097408280118        EUFO
VMAX_2560       vmax    500009740828011c        EUFO
VMAX_2560       vmax    5000097408280120        EUFO
VMAX_2560       vmax    5000097408280124        EUFO
VMAX_2560       vmax    50000974082801dc        EUFO
VMAX_2560       vmax    50000974082801e0        EUFO
VMAX_2560       vmax    5000097408280198        EUFO
VMAX_3453       vmax    500009740835f518        Mercier
VMAX_3453       vmax    500009740835f51c        Mercier
VMAX_3453       vmax    500009740835f520        Mercier
VMAX_3453       vmax    500009740835f524        Mercier
VMAX_3453       vmax    500009740835f5dc        Mercier
VMAX_3453       vmax    500009740835f5e0        Mercier
VMAX_3453       vmax    500009740835f598        Mercier
VNX_0475        vnx     500601643EA02F4A        Mercier
VNX_0476        vnx     500601643ea02f5e        EUFO
VNX_0475        vnx     500601653ea02f4a        Mercier
VNX_0476        vnx     500601653ea02f5e        EUFO
VNX_0475        vnx     500601663EA02F4A        Mercier
VNX_0476        vnx     500601663ea02f5e        EUFO
VNX_0475        vnx     500601673ea02f4a        Mercier
VNX_0476        vnx     500601673ea02f5e        EUFO
VNX_0475        vnx     5006016c3EA02F4A        Mercier
VNX_0476        vnx     5006016c3ea02f5e        EUFO
VNX_0475        vnx     5006016d3ea02f4a        Mercier
VNX_0476        vnx     5006016d3ea02f5e        EUFO
VNX_0475        vnx     5006016e3EA02F4A        Mercier
VNX_0476        vnx     5006016e3ea02f5e        EUFO
VNX_0475        vnx     5006016f3ea02f4a        Mercier
VNX_0476        vnx     5006016f3ea02f5e        EUFO
Dmx2000         dmx2000 5006048C49AEF607        Mercier
Dmx2000         dmx2000 5006048C49AEF608        Mercier
Dmx4            dmx4    5006048C52A80407        EUFO
Dmx4            dmx4    5006048C52A80408        EUFO

0[130523/085049]root@pegase# 







